{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"papermill":{"default_parameters":{},"duration":814.688051,"end_time":"2021-09-30T17:55:00.964263","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-09-30T17:41:26.276212","version":"2.3.3"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":172291,"sourceType":"datasetVersion","datasetId":76821},{"sourceId":8848616,"sourceType":"datasetVersion","datasetId":5326086}],"dockerImageVersionId":12836,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Generating text for news headlines:\nLanguage Modelling is the core problem for a number of of natural language processing tasks such as speech to text, conversational system, and text summarization. A trained language model learns the likelihood of occurrence of a word/ character based on the previous sequence of words/ characters used in the text. Language models can be operated at character level, n-gram level, sentence level or even paragraph level. We will create a language model for predicting next word by implementing and training state-of-the-art Recurrent Neural Networks under Deep Learning.\n\nWe will use **Keras** library in python to implement our project.","metadata":{"_cell_guid":"e084e610-8128-4769-ab64-6aa194044892","_uuid":"20c011dd401be7b6448c43f965e5d0bf548c53b9","id":"64a19297","papermill":{"duration":0.014462,"end_time":"2021-09-30T17:41:32.664182","exception":false,"start_time":"2021-09-30T17:41:32.649720","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### 1. Import the libraries","metadata":{"papermill":{"duration":0.012638,"end_time":"2021-09-30T17:41:32.691710","exception":false,"start_time":"2021-09-30T17:41:32.679072","status":"completed"},"tags":[],"id":"6033f989"}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport string\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"a4a114b9","papermill":{"duration":4.480734,"end_time":"2021-09-30T17:41:37.185324","exception":false,"start_time":"2021-09-30T17:41:32.704590","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-03T10:41:57.793875Z","iopub.execute_input":"2024-07-03T10:41:57.794128Z","iopub.status.idle":"2024-07-03T10:41:57.817612Z","shell.execute_reply.started":"2024-07-03T10:41:57.794076Z","shell.execute_reply":"2024-07-03T10:41:57.816911Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### 2. Load the dataset\n\nLoad the dataset of news headlines","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"3bcfcaab","papermill":{"duration":0.013434,"end_time":"2021-09-30T17:41:37.213527","exception":false,"start_time":"2021-09-30T17:41:37.200093","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\ndef read_file_to_sentences(file_path):\n    with open(file_path, 'r', encoding='utf-8') as file:\n        text = file.read()\n    sentences = sent_tokenize(text)\n    return sentences\nall_headlines = read_file_to_sentences(\"/kaggle/input/corpus2/game_of_thrones.txt\")\nlen(all_headlines)","metadata":{"id":"5896a2f8","outputId":"2598af11-6a37-485a-b359-37a4a7bdfe27","papermill":{"duration":0.072018,"end_time":"2021-09-30T17:41:37.299206","exception":false,"start_time":"2021-09-30T17:41:37.227188","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:41:57.818996Z","iopub.execute_input":"2024-07-03T10:41:57.819286Z","iopub.status.idle":"2024-07-03T10:42:00.451918Z","shell.execute_reply.started":"2024-07-03T10:41:57.819232Z","shell.execute_reply":"2024-07-03T10:42:00.451080Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"24248"},"metadata":{}}]},{"cell_type":"code","source":"all_headlines[:10]","metadata":{"id":"86a8ddd4","outputId":"447561ee-056c-489a-ce95-0fb7ff4d32ec","papermill":{"duration":0.026046,"end_time":"2021-09-30T17:41:37.341958","exception":false,"start_time":"2021-09-30T17:41:37.315912","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:42:09.566469Z","iopub.execute_input":"2024-07-03T10:42:09.566760Z","iopub.status.idle":"2024-07-03T10:42:09.572111Z","shell.execute_reply.started":"2024-07-03T10:42:09.566715Z","shell.execute_reply":"2024-07-03T10:42:09.571433Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['A Song of Ice and Fire\\n\\nA Game of Thrones\\n\\nPROLOGUE\\n\\nWe should start back, Gared urged as the woods began to grow dark around them.',\n 'The wildlings are dead.',\n 'Do the dead frighten you?',\n 'Ser Waymar Royce asked with just the hint of a smile.',\n 'Gared did not rise to the bait.',\n 'He was an old man, past fifty, and he had seen the lordlings come and go.',\n 'Dead is dead, he said.',\n 'We have no business with the dead.',\n 'Are they dead?',\n 'Royce asked softly.']"},"metadata":{}}]},{"cell_type":"markdown","source":"### 3. Dataset preparation\n\n#### 3.1 Dataset cleaning\n\nIn dataset preparation step, we will first perform text cleaning of the data which includes removal of punctuations and lower casing all the words.","metadata":{"_cell_guid":"9dbd8bc9-fb61-43b9-b0c4-98bd7f3f8150","_uuid":"fda5d4868631d3618d4d9a9a863541b2faf121c0","id":"e11f544d","papermill":{"duration":0.015287,"end_time":"2021-09-30T17:41:37.374583","exception":false,"start_time":"2021-09-30T17:41:37.359296","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import string\n\ndef clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt\n\ncorpus = [clean_text(x) for x in all_headlines]\ncorpus[:10]","metadata":{"_cell_guid":"b8bf84ed-da11-4f89-a584-9dceea677420","_uuid":"2a07365a27a7ba2f92fc9ba4d05d8e6254a68d8c","id":"273f39b8","outputId":"ce85ef69-c5bc-4c8c-e06b-f54b5887b4a9","papermill":{"duration":0.034873,"end_time":"2021-09-30T17:41:37.424319","exception":false,"start_time":"2021-09-30T17:41:37.389446","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:42:12.099934Z","iopub.execute_input":"2024-07-03T10:42:12.100212Z","iopub.status.idle":"2024-07-03T10:42:12.442976Z","shell.execute_reply.started":"2024-07-03T10:42:12.100171Z","shell.execute_reply":"2024-07-03T10:42:12.442259Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"['a song of ice and fire\\n\\na game of thrones\\n\\nprologue\\n\\nwe should start back gared urged as the woods began to grow dark around them',\n 'the wildlings are dead',\n 'do the dead frighten you',\n 'ser waymar royce asked with just the hint of a smile',\n 'gared did not rise to the bait',\n 'he was an old man past fifty and he had seen the lordlings come and go',\n 'dead is dead he said',\n 'we have no business with the dead',\n 'are they dead',\n 'royce asked softly']"},"metadata":{}}]},{"cell_type":"markdown","source":"#### 3.2 Make an id for each word in corpus\nThe next step is Tokenization. Tokenization is a process of extracting tokens (terms / words) from a corpus. Pythonâ€™s library Keras has inbuilt model for tokenization which can be used to obtain the tokens and their index in the corpus. After this step, every text document in the dataset is converted into sequence of tokens.\n","metadata":{"_cell_guid":"9d83cc08-19ba-4b00-9ca6-dcf5ff39c8af","_uuid":"6fd11859fd71aa5c7ce10bdbbd31c8eb6d1b3118","id":"b9947f82","papermill":{"duration":0.014495,"end_time":"2021-09-30T17:41:37.453626","exception":false,"start_time":"2021-09-30T17:41:37.439131","status":"completed"},"tags":[]}},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ntoken_list = tokenizer.texts_to_sequences([\"I am happy to see you here today\"])[0]\nprint(token_list)\n\ncheck=[]\n\nfor i in range(1, len(token_list)):\n  n_gram_sequence = token_list[:i+1]\n  check.append(n_gram_sequence)","metadata":{"id":"8ddba2e7","outputId":"f10775a7-1140-4c8f-a6f4-6ea8c5bed7b0","papermill":{"duration":0.048788,"end_time":"2021-09-30T17:41:37.517111","exception":false,"start_time":"2021-09-30T17:41:37.468323","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:42:15.280007Z","iopub.execute_input":"2024-07-03T10:42:15.280285Z","iopub.status.idle":"2024-07-03T10:42:15.869935Z","shell.execute_reply.started":"2024-07-03T10:42:15.280244Z","shell.execute_reply":"2024-07-03T10:42:15.869172Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"[15, 187, 2292, 3, 77, 10, 83, 831]\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer()\n\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n\n    ## convert data to sequence of tokens\n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\n\ninp_sequences[:10], total_words","metadata":{"_cell_guid":"896543c9-7944-4748-b8ef-ef8cbc2a84f0","_uuid":"9129a8b773feb72eff91aa0025157a173d10c625","id":"e34ee397","outputId":"4e7d2a5d-61db-425d-f61d-210cda43314d","papermill":{"duration":0.063827,"end_time":"2021-09-30T17:41:37.596199","exception":false,"start_time":"2021-09-30T17:41:37.532372","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:42:17.900045Z","iopub.execute_input":"2024-07-03T10:42:17.900337Z","iopub.status.idle":"2024-07-03T10:42:19.497961Z","shell.execute_reply.started":"2024-07-03T10:42:17.900294Z","shell.execute_reply":"2024-07-03T10:42:19.497125Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"([[4, 1031],\n  [4, 1031, 5],\n  [4, 1031, 5, 553],\n  [4, 1031, 5, 553, 2],\n  [4, 1031, 5, 553, 2, 256],\n  [4, 1031, 5, 553, 2, 256, 4],\n  [4, 1031, 5, 553, 2, 256, 4, 1293],\n  [4, 1031, 5, 553, 2, 256, 4, 1293, 5],\n  [4, 1031, 5, 553, 2, 256, 4, 1293, 5, 2085],\n  [4, 1031, 5, 553, 2, 256, 4, 1293, 5, 2085, 7573]],\n 12116)"},"metadata":{}}]},{"cell_type":"markdown","source":"In the above output [1119, 1120], [1119, 1120,116], [1119, 1120, 116, 1121] and so on represents the ngram phrases generated from the input data. where every integer corresponds to the index of a particular word in the complete vocabulary of words present in the text. For example\n\n**Headline:** i stand  with the shedevils  \n**Ngrams:** | **Sequence of Tokens**\n\n<table>\n<tr><td>Ngram </td><td> Sequence of Tokens</td></tr>\n<tr> <td>i stand </td><td> [30, 507] </td></tr>\n<tr> <td>i stand with </td><td> [30, 507, 11] </td></tr>\n<tr> <td>i stand with the </td><td> [30, 507, 11, 1] </td></tr>\n<tr> <td>i stand with the shedevils </td><td> [30, 507, 11, 1, 975] </td></tr>\n</table>\n\n\n\n#### 3.3 Padding the Sequences and obtain Variables : Predictors and Target\nNow we have generated a data-set which contains sequence of tokens. Before starting training the model, we need to pad the sequences and make their lengths equal. We can use pad_sequence function of Kears for this purpose. To input this data into a learning model, we need to create predictors and label. For example:\n\n\nHeadline:  they are learning data science\n\n<table>\n<tr><td>PREDICTORS </td> <td>           LABEL </td></tr>\n<tr><td>they                   </td> <td>  are</td></tr>\n<tr><td>they are               </td> <td>  learning</td></tr>\n<tr><td>they are learning      </td> <td>  data</td></tr>\n<tr><td>they are learning data </td> <td>  science</td></tr>\n</table>","metadata":{"_cell_guid":"a22c88f5-f2a3-457c-835b-63341e657e3f","_uuid":"f22aa5e0c04620ca5034ab9389322eee543060c6","id":"5177d0e1","papermill":{"duration":0.014885,"end_time":"2021-09-30T17:41:37.626148","exception":false,"start_time":"2021-09-30T17:41:37.611263","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"####Pad all sentences from start to max length , make labels one hot encoded\n\nif input_sequences = [[0, 0, 1, 2, 3], [0, 2, 3, 4, 5], [0, 0, 0, 3, 4]]\nthen\npredictors will be all elements except last element :\npredictors = [[0, 0, 1, 2], [0, 2, 3, 4], [0, 0, 0, 3]]\nwhile labels will be the last element :\nlabel = [3, 5, 4]\nthen\nlabel = [\n  [0, 0, 0, 1, 0, 0],  # for label 3\n  [0, 0, 0, 0, 0, 1],  # for label 5\n  [0, 0, 0, 0, 1, 0]   # for label 4\n]\n","metadata":{"id":"EfxRLOaS5mIf"}},{"cell_type":"code","source":"def generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)\npredictors,label,len(label[0]),max_sequence_len","metadata":{"_cell_guid":"73254551-40bd-45b1-a7a5-88fe4cbe0b20","_uuid":"ca588b414e70e21bebcead960f6632805d37dd8c","id":"716b957a","outputId":"592eb578-edae-4559-9c12-f40daf8b957a","papermill":{"duration":0.096386,"end_time":"2021-09-30T17:41:37.737572","exception":false,"start_time":"2021-09-30T17:41:37.641186","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:42:24.199144Z","iopub.execute_input":"2024-07-03T10:42:24.199441Z","iopub.status.idle":"2024-07-03T10:42:26.975325Z","shell.execute_reply.started":"2024-07-03T10:42:24.199396Z","shell.execute_reply":"2024-07-03T10:42:26.974499Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(array([[   0,    0,    0, ...,    0,    0,    4],\n        [   0,    0,    0, ...,    0,    4, 1031],\n        [   0,    0,    0, ...,    4, 1031,    5],\n        ...,\n        [   0,    0,    0, ...,   10,   11,    7],\n        [   0,    0,    0, ...,   11,    7, 1382],\n        [   0,    0,    0, ...,    7, 1382, 1026]], dtype=int32),\n array([[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n 12116,\n 131)"},"metadata":{}}]},{"cell_type":"markdown","source":"Perfect, now we can obtain the input vector X (predictors) and the label vector Y (label) which can be used for the training purposes. Now we will create RNN model for out data.\n\n### 4. RNN for Text Generation\nUnlike Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent Neural Networks. This creates loops in the neural network architecture which acts as a â€˜memory stateâ€™ of the neurons. This state allows the neurons an ability to remember what have been learned so far.\n\nThe memory state in RNNs gives an advantage over traditional neural networks. Lets architecture a RNN model in our code. We have added total three layers in the model.\n\n1. Input Layer : Takes the sequence of words as input\n2. RNN Layer : Computes the output using RNN units. We have added 200 units in the layer.\n3. Dropout Layer : A regularisation layer which randomly turns-off the activations of some neurons in the RNN layer. It helps in preventing over fitting.\n4. Output Layer : Computes the probability of the best possible next word as output\n\nWe will run this model for total 100 epoochs but it can be experimented further.","metadata":{"_cell_guid":"8b5d80ff-54a8-4380-8a3c-149be880551d","_uuid":"8b8a64b96011f427c48d5b0819e3e74af604ce43","id":"3bdafcc8","papermill":{"duration":0.015715,"end_time":"2021-09-30T17:41:37.769350","exception":false,"start_time":"2021-09-30T17:41:37.753635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nimport numpy as np\n\n# Define the dataset\nclass TextDataset(Dataset):\n    def __init__(self, predictors, labels):\n        self.predictors = predictors\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.predictors)\n\n    def __getitem__(self, idx):\n        return self.predictors[idx], self.labels[idx]\n\n# Define the model class\nclass TextGenerationModel(nn.Module):\n    def __init__(self, total_words, embedding_dim, rnn_units):\n        super(TextGenerationModel, self).__init__()\n        self.embedding = nn.Embedding(total_words, embedding_dim)\n        self.rnn = nn.RNN(embedding_dim, rnn_units, batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n        self.fc = nn.Linear(rnn_units, total_words)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        x, _ = self.rnn(x)\n        x = self.dropout(x[:, -1, :])  # Only take the output from the last time step\n        x = self.fc(x)\n        return x\n\n# Function to train the model\ndef train_model(model, dataset, epochs, batch_size, learning_rate, patience, device):\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=patience, min_lr=1e-5)\n\n    model.train()\n    for epoch in range(epochs):\n        epoch_loss = 0\n        for batch_idx, (data, target) in enumerate(dataloader):\n            data, target = data.to(device), target.to(device)\n            \n            optimizer.zero_grad()\n            output = model(data)\n\n            # Flatten the output and target for the loss function\n            output = output.view(-1, output.size(-1))\n            target = target.view(-1)\n            \n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n\n        avg_epoch_loss = epoch_loss / len(dataloader)\n        print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}')\n        scheduler.step(avg_epoch_loss)\n\n# Parameters\ntotal_words = 10000  # Example value; use the actual value from your data\nembedding_dim = 32\nrnn_units = 200\nmax_sequence_len = 50  # Example value; use the actual value from your data\nbatch_size = 32\nlearning_rate = 0.001\nepochs = 100\npatience = 5\n\n# Example data (replace with actual data)\npredictors = np.random.randint(0, total_words, (1000, max_sequence_len-1))  # Dummy data\nlabels = np.random.randint(0, total_words, 1000)  # Dummy data\n\n# Convert data to tensors\npredictors = torch.tensor(predictors, dtype=torch.long)\nlabels = torch.tensor(labels, dtype=torch.long)\n\n# Check tensor shapes and device\nprint(f\"Predictors shape: {predictors.shape}, Labels shape: {labels.shape}\")\nprint(f\"Predictors device: {predictors.device}, Labels device: {labels.device}\")\n\n# Prepare dataset\ndataset = TextDataset(predictors, labels)\n\n# Initialize and train the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = TextGenerationModel(total_words, embedding_dim, rnn_units).to(device)\n\n# Train the model\ntrain_model(model, dataset, epochs, batch_size, learning_rate, patience, device)\n","metadata":{"_cell_guid":"60d6721e-e40e-4f2b-8f63-c06459d68f26","_uuid":"76ef6d9352002d333a7c75e8aed7ce996015f527","id":"dc83c566","outputId":"271692f9-8ea1-401f-c74a-764f3aae50a5","papermill":{"duration":2.114226,"end_time":"2021-09-30T17:41:39.898971","exception":false,"start_time":"2021-09-30T17:41:37.784745","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T10:48:46.502690Z","iopub.execute_input":"2024-07-03T10:48:46.502989Z","iopub.status.idle":"2024-07-03T10:48:59.293357Z","shell.execute_reply.started":"2024-07-03T10:48:46.502946Z","shell.execute_reply":"2024-07-03T10:48:59.292681Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Predictors shape: torch.Size([1000, 49]), Labels shape: torch.Size([1000])\nPredictors device: cpu, Labels device: cpu\nEpoch 1/100, Loss: 9.2294\nEpoch 2/100, Loss: 7.9520\nEpoch 3/100, Loss: 7.0736\nEpoch 4/100, Loss: 6.5865\nEpoch 5/100, Loss: 6.1579\nEpoch 6/100, Loss: 5.6549\nEpoch 7/100, Loss: 5.0389\nEpoch 8/100, Loss: 4.3272\nEpoch 9/100, Loss: 3.5775\nEpoch 10/100, Loss: 2.8011\nEpoch 11/100, Loss: 2.0762\nEpoch 12/100, Loss: 1.4474\nEpoch 13/100, Loss: 0.9579\nEpoch 14/100, Loss: 0.6314\nEpoch 15/100, Loss: 0.4311\nEpoch 16/100, Loss: 0.3157\nEpoch 17/100, Loss: 0.2334\nEpoch 18/100, Loss: 0.1845\nEpoch 19/100, Loss: 0.1508\nEpoch 20/100, Loss: 0.1265\nEpoch 21/100, Loss: 0.1079\nEpoch 22/100, Loss: 0.0921\nEpoch 23/100, Loss: 0.0818\nEpoch 24/100, Loss: 0.0743\nEpoch 25/100, Loss: 0.0635\nEpoch 26/100, Loss: 0.0591\nEpoch 27/100, Loss: 0.0538\nEpoch 28/100, Loss: 0.0492\nEpoch 29/100, Loss: 0.0449\nEpoch 30/100, Loss: 0.0409\nEpoch 31/100, Loss: 0.0372\nEpoch 32/100, Loss: 0.0353\nEpoch 33/100, Loss: 0.0325\nEpoch 34/100, Loss: 0.0314\nEpoch 35/100, Loss: 0.0291\nEpoch 36/100, Loss: 0.0280\nEpoch 37/100, Loss: 0.0260\nEpoch 38/100, Loss: 0.0239\nEpoch 39/100, Loss: 0.0233\nEpoch 40/100, Loss: 0.0218\nEpoch 41/100, Loss: 0.0203\nEpoch 42/100, Loss: 0.0190\nEpoch 43/100, Loss: 0.0186\nEpoch 44/100, Loss: 0.0178\nEpoch 45/100, Loss: 0.0170\nEpoch 46/100, Loss: 0.0161\nEpoch 47/100, Loss: 0.0151\nEpoch 48/100, Loss: 0.0146\nEpoch 49/100, Loss: 0.0141\nEpoch 50/100, Loss: 0.0133\nEpoch 51/100, Loss: 0.0128\nEpoch 52/100, Loss: 0.0126\nEpoch 53/100, Loss: 0.0120\nEpoch 54/100, Loss: 0.0116\nEpoch 55/100, Loss: 0.0112\nEpoch 56/100, Loss: 0.0107\nEpoch 57/100, Loss: 0.0101\nEpoch 58/100, Loss: 0.0098\nEpoch 59/100, Loss: 0.0095\nEpoch 60/100, Loss: 0.0091\nEpoch 61/100, Loss: 0.0088\nEpoch 62/100, Loss: 0.0087\nEpoch 63/100, Loss: 0.0082\nEpoch 64/100, Loss: 0.0081\nEpoch 65/100, Loss: 0.0079\nEpoch 66/100, Loss: 0.0076\nEpoch 67/100, Loss: 0.0073\nEpoch 68/100, Loss: 0.0072\nEpoch 69/100, Loss: 0.0068\nEpoch 70/100, Loss: 0.0066\nEpoch 71/100, Loss: 0.0064\nEpoch 72/100, Loss: 0.0062\nEpoch 73/100, Loss: 0.0060\nEpoch 74/100, Loss: 0.0059\nEpoch 75/100, Loss: 0.0057\nEpoch 76/100, Loss: 0.0056\nEpoch 77/100, Loss: 0.0054\nEpoch 78/100, Loss: 0.0052\nEpoch 79/100, Loss: 0.0051\nEpoch 80/100, Loss: 0.0050\nEpoch 81/100, Loss: 0.0048\nEpoch 82/100, Loss: 0.0047\nEpoch 83/100, Loss: 0.0046\nEpoch 84/100, Loss: 0.0044\nEpoch 85/100, Loss: 0.0043\nEpoch 86/100, Loss: 0.0042\nEpoch 87/100, Loss: 0.0042\nEpoch 88/100, Loss: 0.0041\nEpoch 89/100, Loss: 0.0040\nEpoch 90/100, Loss: 0.0038\nEpoch 91/100, Loss: 0.0037\nEpoch 92/100, Loss: 0.0037\nEpoch 93/100, Loss: 0.0035\nEpoch 94/100, Loss: 0.0036\nEpoch 95/100, Loss: 0.0034\nEpoch 96/100, Loss: 0.0033\nEpoch 97/100, Loss: 0.0032\nEpoch 98/100, Loss: 0.0032\nEpoch 99/100, Loss: 0.0031\nEpoch 100/100, Loss: 0.0030\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Lets train our model now","metadata":{"_cell_guid":"1826aa1a-cb77-4379-a69d-e9b180945dce","_uuid":"f0b16b471969dbb831cb0024e303341e11b63de4","id":"b2113e33","papermill":{"duration":0.015837,"end_time":"2021-09-30T17:41:39.931660","exception":false,"start_time":"2021-09-30T17:41:39.915823","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model.fit(predictors, label, epochs=8)","metadata":{"_cell_guid":"07d5cf03-d171-4993-9f8b-18446649ecb0","_uuid":"156f3303b8120cc6932e6db985cbea4a7ceb08bf","id":"261be263","papermill":{"duration":756.537974,"end_time":"2021-09-30T17:54:16.485662","exception":false,"start_time":"2021-09-30T17:41:39.947688","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-03T10:25:11.882795Z","iopub.execute_input":"2024-07-03T10:25:11.883071Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/8\n260626/260626 [==============================] - 532s 2ms/step - loss: 6.5739\nEpoch 2/8\n205536/260626 [======================>.......] - ETA: 1:53 - loss: 5.9637","output_type":"stream"}]},{"cell_type":"markdown","source":"## 5. Generating the text\n\nGreat, our model architecture is now ready and we can train it using our data. Next lets write the function to predict the next word based on the input words (or seed text). We will first tokenize the seed text, pad the sequences and pass into the trained model to get predicted word. The multiple predicted words can be appended together to get predicted sequence.\n","metadata":{"_cell_guid":"61e99cfe-7395-4d61-8d1a-8539103d3db5","_uuid":"448bf43b123060dfe4e27cb9f12889e4fe0ed2a7","id":"52cab9f1","papermill":{"duration":3.529664,"end_time":"2021-09-30T17:54:23.496285","exception":false,"start_time":"2021-09-30T17:54:19.966621","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nfrom keras.preprocessing.sequence import pad_sequences\nimport numpy as np\n\ndef generate_text(seed_text, next_words, model, tokenizer, max_sequence_len, device):\n    model.eval()  # Set the model to evaluation mode\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        token_list = torch.tensor(token_list, dtype=torch.long).to(device)\n        \n        with torch.no_grad():\n            predicted_probs = model(token_list)\n        \n        predicted = torch.argmax(predicted_probs, dim=-1).item()  # Get the index of the highest probability\n\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return seed_text.title()\n\n# Ensure the model, tokenizer, and device are already defined\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"_uuid":"e71e56543b7065f115a05e3fd062262b3b94ad46","id":"ea25320f","papermill":{"duration":3.507383,"end_time":"2021-09-30T17:54:30.746176","exception":false,"start_time":"2021-09-30T17:54:27.238793","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-03T10:58:21.409245Z","iopub.execute_input":"2024-07-03T10:58:21.409585Z","iopub.status.idle":"2024-07-03T10:58:21.419941Z","shell.execute_reply.started":"2024-07-03T10:58:21.409518Z","shell.execute_reply":"2024-07-03T10:58:21.418967Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"## 6. Some Results","metadata":{"_cell_guid":"ea0bddb6-acc6-4592-a2e0-ffc4129a582f","_uuid":"c49bf4ea0e54f3145149e164e243d897f545b84c","id":"d2be725b","papermill":{"duration":4.274927,"end_time":"2021-09-30T17:54:38.541384","exception":false,"start_time":"2021-09-30T17:54:34.266457","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print(generate_text(\"Have \", 5, model, tokenizer, max_sequence_len, device))\nprint(generate_text(\"Did\", 5, model, tokenizer, max_sequence_len, device))\nprint(generate_text(\"We\", 6, model, tokenizer, max_sequence_len, device))\nprint(generate_text(\" All the bodies\", 3, model, tokenizer, max_sequence_len, device))\nprint(generate_text(\"Especially\", 4, model, tokenizer, max_sequence_len, device))\nprint(generate_text(\"The young knight\", 6, model, tokenizer, max_sequence_len, device))","metadata":{"_cell_guid":"e38dd280-093b-4091-b82b-9aa90045b107","_kg_hide-input":true,"_uuid":"a21548224c9e661a29e3d369e348aada0599bdc9","id":"6fb05b0b","outputId":"196598e0-5c4e-4665-805f-4e40ef79b59f","papermill":{"duration":4.617206,"end_time":"2021-09-30T17:54:46.708976","exception":false,"start_time":"2021-09-30T17:54:42.091770","status":"completed"},"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-07-03T11:02:44.420335Z","iopub.execute_input":"2024-07-03T11:02:44.420625Z","iopub.status.idle":"2024-07-03T11:02:44.486951Z","shell.execute_reply.started":"2024-07-03T11:02:44.420582Z","shell.execute_reply":"2024-07-03T11:02:44.486154Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Have  Fiery Bends Sneaking Eighth Favored\nDid Magister Mornings Humpbacked Stalking Debt\nWe Glistened Bends Startledand Shields Hallooed Claws\n All The Bodies Favored Outrange Obey\nEspecially Tale List Tide Impressive\nThe Young Knight Partner Fashioned Helplessly Lemon Reward Notched\n","output_type":"stream"}]},{"cell_type":"code","source":"print(generate_text(\"Nobody will feel\", 4, model, tokenizer, max_sequence_len, device))","metadata":{"id":"ZLthwZpX9vNd","execution":{"iopub.status.busy":"2024-07-03T11:06:04.103512Z","iopub.execute_input":"2024-07-03T11:06:04.103788Z","iopub.status.idle":"2024-07-03T11:06:04.118537Z","shell.execute_reply.started":"2024-07-03T11:06:04.103746Z","shell.execute_reply":"2024-07-03T11:06:04.117686Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Nobody Will Feel Lolling Outside Silverpale Leffords\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}