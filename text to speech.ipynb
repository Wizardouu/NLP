{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers==4.35.2\n",
    "!pip install datasets==2.15.0\n",
    "!pip install soundfile==0.12.1\n",
    "!pip install speechbrain==0.5.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:09:26.518646Z",
     "iopub.status.busy": "2024-12-21T05:09:26.518332Z",
     "iopub.status.idle": "2024-12-21T05:09:26.523467Z",
     "shell.execute_reply": "2024-12-21T05:09:26.522525Z",
     "shell.execute_reply.started": "2024-12-21T05:09:26.518622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "\n",
    "# Replace 'your_token_here' with your actual Hugging Face token\n",
    "token = \"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\"\n",
    "HfFolder.save_token(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:06:49.876385Z",
     "iopub.status.busy": "2024-12-21T05:06:49.875545Z",
     "iopub.status.idle": "2024-12-21T05:09:26.516698Z",
     "shell.execute_reply": "2024-12-21T05:09:26.515784Z",
     "shell.execute_reply.started": "2024-12-21T05:06:49.876335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset,Dataset, Audio\n",
    "\n",
    "dataset_stream = load_dataset(\"facebook/voxpopuli\", \"en\", streaming=True)\n",
    "\n",
    "dataset = {\n",
    "    'train': dataset_stream[\"train\"],\n",
    "    'validation': dataset_stream[\"validation\"],\n",
    "    'test': dataset_stream[\"test\"]\n",
    "}\n",
    "\n",
    "# Cast the audio column for preprocessing\n",
    "dataset['train'] = dataset['train'].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset['validation'] = dataset['validation'].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "dataset['test'] = dataset['test'].cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "\n",
    "train_dataset = list(dataset_stream[\"train\"].take(5500))\n",
    "validation_dataset = list(dataset_stream[\"validation\"].take(500))\n",
    "test_dataset = list(dataset_stream[\"test\"].take(500))\n",
    "\n",
    "dataset = {\n",
    "    'train': Dataset.from_list(train_dataset),\n",
    "    'validation': Dataset.from_list(validation_dataset),\n",
    "    'test': Dataset.from_list(test_dataset)\n",
    "}\n",
    "train_dataset = dataset['train']\n",
    "valid_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:09:26.525177Z",
     "iopub.status.busy": "2024-12-21T05:09:26.524600Z",
     "iopub.status.idle": "2024-12-21T05:09:29.302204Z",
     "shell.execute_reply": "2024-12-21T05:09:29.301296Z",
     "shell.execute_reply.started": "2024-12-21T05:09:26.525142Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b680352830054051b0b36d3282630ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8696768a3a84e90990d720c955acb83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/232 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01debc675eed4fd1b22cf18f14ae9c55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm_char.model:   0%|          | 0.00/238k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0e0394f2724fb28bc9a8477590999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/40.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332874cb63e24b759458c6085e4ecdef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint=\"microsoft/speecht5_tts\"\n",
    "processor=SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:09:29.304794Z",
     "iopub.status.busy": "2024-12-21T05:09:29.304209Z",
     "iopub.status.idle": "2024-12-21T05:09:29.352755Z",
     "shell.execute_reply": "2024-12-21T05:09:29.351899Z",
     "shell.execute_reply.started": "2024-12-21T05:09:29.304760Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4d2ece93e94b719f3eb8958c0477e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text=\" \".join(batch[\"normalized_text\"])\n",
    "    vocab=list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\":[all_text]}\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "if isinstance(train_dataset, dict):\n",
    "    train_dataset = Dataset.from_dict(train_dataset)\n",
    "\n",
    "# Now you can use dataset.column_names\n",
    "vocabs = train_dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer=processor.tokenizer\n",
    "\n",
    "dataset_vocab=set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab={k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-21T05:09:29.353981Z",
     "iopub.status.busy": "2024-12-21T05:09:29.353734Z",
     "iopub.status.idle": "2024-12-21T05:09:31.689157Z",
     "shell.execute_reply": "2024-12-21T05:09:31.688417Z",
     "shell.execute_reply.started": "2024-12-21T05:09:29.353959Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4f5cd8d4814747aa549aec96a60dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "hyperparams.yaml:   0%|          | 0.00/2.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a087ed6a062f4f60871b9fe44d0ee3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "embedding_model.ckpt:   0%|          | 0.00/16.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1140db264444218765d2871165ea18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "mean_var_norm_emb.ckpt:   0%|          | 0.00/3.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dfd9d26c8040e08f2f034533fef860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "classifier.ckpt:   0%|          | 0.00/15.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf598503ff4430da97266feb86e8221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "label_encoder.txt:   0%|          | 0.00/129k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "\n",
    "spk_model_name=\"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device=\"cuda\"\n",
    "speaker_model=EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings=speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings=torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings=speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings\n",
    "\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio=example[\"audio\"]\n",
    "    \n",
    "    example=processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    \n",
    "    # strip off thje batch dimension\n",
    "    example[\"labels\"]=example[\"labels\"][0]\n",
    "    \n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"]=create_speaker_embedding(audio[\"array\"])\n",
    "    \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2024-12-21T05:20:23.905Z",
     "iopub.execute_input": "2024-12-21T05:09:31.690749Z",
     "iopub.status.busy": "2024-12-21T05:09:31.690416Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d3add370964f11babb90bfa9d38cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 600). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# applying the processing function to the entire dataset\n",
    "train_dataset=train_dataset.map(prepare_dataset)\n",
    "valid_dataset=valid_dataset.map(prepare_dataset)\n",
    "test_dataset=test_dataset.map(prepare_dataset)\n",
    "# Columns to remove\n",
    "columns_to_remove = [\n",
    "    \"audio_id\",\n",
    "    \"language\",\n",
    "    \"audio\",\n",
    "    \"raw_text\",\n",
    "    \"normalized_text\",\n",
    "    \"gender\",\n",
    "    \"speaker_id\",\n",
    "    \"is_gold_transcript\",\n",
    "    \"accent\",\n",
    "]\n",
    "\n",
    "# Remove columns\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)\n",
    "valid_dataset = valid_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    input_length=len(input_ids)\n",
    "    return input_length<220\n",
    "\n",
    "train_dataset=train_dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "test_dataset=test_dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "valid_dataset=valid_dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    \n",
    "    processor: Any\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids=[{\"input_ids\":feature[\"input_ids\"]} for feature in features]\n",
    "        label_features=[{\"input_values\":feature[\"labels\"]} for feature in features]\n",
    "        speaker_features=[feature[\"speaker_embeddings\"] for feature in features]\n",
    "        \n",
    "        # collate the inputs and targets into a batch\n",
    "        batch=processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n",
    "        \n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"]=batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1),-100)\n",
    "        #not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "        \n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor>1:\n",
    "            target_lengths=torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n",
    "            target_lengths=target_lengths.new(\n",
    "                [length-length%model.config.reduction_factor for length in target_lengths]\n",
    "            )\n",
    "            max_length=max(target_lengths)\n",
    "            batch[\"labels\"]=batch[\"labels\"][:, :max_length]\n",
    "        \n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"]=torch.tensor(speaker_features)\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_collator=TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model=SpeechT5ForTextToSpeech.from_pretrained(checkpoint)\n",
    "model.config.use_cache=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"sohail3\",                       \n",
    "    per_device_train_batch_size=8,               \n",
    "    gradient_accumulation_steps=8,               \n",
    "    learning_rate=0.0002,                        \n",
    "    weight_decay=0.01,                           \n",
    "    warmup_steps=50,                             \n",
    "    num_train_epochs=3,                          \n",
    "    gradient_checkpointing=True,                 \n",
    "    evaluation_strategy=\"steps\",                 \n",
    "    per_device_eval_batch_size=2,                \n",
    "    save_steps=100,                              \n",
    "    eval_steps=50,                               \n",
    "    logging_steps=25,                            \n",
    "    load_best_model_at_end=True,                 \n",
    "    greater_is_better=False,                     \n",
    "    label_names=[\"labels\"],                      \n",
    "    push_to_hub=False,                           \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=processor,                         \n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('./my_model_sohail1')\n",
    "processor.save_pretrained('./my_model_sohail1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import upload_folder\n",
    "\n",
    "# Specify the model path and your Hugging Face repository name\n",
    "upload_folder(\n",
    "    repo_id=\"sohail2003/pattern3.1\",\n",
    "    folder_path=\"/kaggle/working/my_model_sohail1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "import torchaudio\n",
    "\n",
    "# Path to your fine-tuned checkpoint\n",
    "checkpoint_path = \"/kaggle/working/sohail/checkpoint-200\"\n",
    "\n",
    "# Load the processor and TTS model\n",
    "processor = SpeechT5Processor.from_pretrained(\"sohail2003/pattern3\")\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"sohail2003/pattern3\")\n",
    "\n",
    "# Load HiFi-GAN vocoder\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")\n",
    "\n",
    "# Load SpeechBrain speaker embedding model\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "# Function to generate speaker embeddings\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform).to(device))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        return speaker_embeddings.squeeze().cpu().numpy()\n",
    "\n",
    "# Example dataset preparation (if needed for testing)\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "    # Strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # Use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "    return example\n",
    "\n",
    "def generate_random_speaker_embedding():\n",
    "    # The embedding size for SpeechT5 is 512 dimensions\n",
    "    return torch.randn(1, 512).float()\n",
    "\n",
    "# Generate TTS with random speaker embedding\n",
    "# Function to generate speech from text using a random speaker embedding\n",
    "def generate_speech(text):\n",
    "    # Create a random speaker embedding\n",
    "    speaker_embedding = generate_random_speaker_embedding()\n",
    "\n",
    "    # Prepare the text input\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate speech\n",
    "    speech = model.generate_speech(inputs[\"input_ids\"], speaker_embedding, vocoder=vocoder)\n",
    "\n",
    "    # Reshape the speech tensor to ensure it's 2D [channels, samples]\n",
    "    if speech.dim() == 1:  # Check if the tensor is 1D (single-channel audio)\n",
    "        speech = speech.unsqueeze(0)  # Add a channel dimension\n",
    "\n",
    "    # Save the speech to a file\n",
    "    torchaudio.save(\"output.wav\", speech, 16000)\n",
    "    print(\"Speech saved to 'output.wav'\")\n",
    "    return speech\n",
    "\n",
    "\n",
    "# Example usage\n",
    "text_input = \"why are you doing that are you crazy \"\n",
    "# Generate speech\n",
    "generated_speech = generate_speech(text_input)\n",
    "\n",
    "# Play the audio (optional)\n",
    "from IPython.display import Audio\n",
    "Audio(\"output.wav\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
