{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-21T12:59:45.438260Z",
     "iopub.status.busy": "2025-05-21T12:59:45.437464Z",
     "iopub.status.idle": "2025-05-21T13:01:34.253340Z",
     "shell.execute_reply": "2025-05-21T13:01:34.251415Z",
     "shell.execute_reply.started": "2025-05-21T12:59:45.438227Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.10.0)\n",
      "Requirement already satisfied: rouge_score in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.1.2)\n",
      "Requirement already satisfied: neo4j in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (5.28.1)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (0.3.20)\n",
      "Requirement already satisfied: sentence-transformers in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (4.46.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: absl-py in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: pytz in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from neo4j) (2024.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (0.3.49)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (0.3.21)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (3.10.10)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (2.8.1)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (0.3.13)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.16.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain<1.0.0,>=0.3.21->langchain_community) (2.10.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (1.33)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langchain-core<1.0.0,>=0.3.45->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain_community) (2.27.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (0.26.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ADMIN\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu rouge_score neo4j langchain_community sentence-transformers transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please enter Your Groq Api Key and Your NEO4J credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:43.380776Z",
     "iopub.status.busy": "2025-05-21T13:01:43.379735Z",
     "iopub.status.idle": "2025-05-21T13:01:46.146450Z",
     "shell.execute_reply": "2025-05-21T13:01:46.144952Z",
     "shell.execute_reply.started": "2025-05-21T13:01:43.380683Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from dateutil import parser\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "from neo4j import GraphDatabase\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import calendar\n",
    "import faiss\n",
    "import spacy\n",
    "import ast\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_value_0 = user_secrets.get_secret(\"NEO4J_PASSWORD\")\n",
    "secret_value_1 = user_secrets.get_secret(\"NEO4J_USERNAME\")\n",
    "secret_value = UserSecretsClient().get_secret(\"GROQ_API_KEY\")\n",
    "GROQ_API_KEY = secret_value\n",
    "MODEL = \"llama3-70b-8192\"   \n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "NEO4J_URI = \"neo4j+s://5d3576c6.databases.neo4j.io\"  # Or your Neo4j Aura URI\n",
    "NEO4J_USER = secret_value_1\n",
    "NEO4J_PASSWORD = secret_value_0\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code for  Manual Graph  Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:53.660207Z",
     "iopub.status.busy": "2025-05-21T13:01:53.659744Z",
     "iopub.status.idle": "2025-05-21T13:01:53.708625Z",
     "shell.execute_reply": "2025-05-21T13:01:53.706908Z",
     "shell.execute_reply.started": "2025-05-21T13:01:53.660175Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, subject):\n",
    "        self.subject = subject\n",
    "        self.edges = defaultdict(list)  # predicate -> list of objects\n",
    "        self.predicate_embeddings = {}  # predicate -> embedding vector\n",
    "\n",
    "    def add_edge(self, predicate, object_, embedder):\n",
    "        self.edges[predicate].append(object_)\n",
    "        if predicate not in self.predicate_embeddings:\n",
    "            self.predicate_embeddings[predicate] = embedder.encode(predicate, convert_to_numpy=True)\n",
    "\n",
    "class Hash:\n",
    "    capacity = 10000\n",
    "    similarity_threshold = 0.5\n",
    "    \n",
    "    def __init__(self, initial_date):\n",
    "        self.start_index = self.date_to_value(initial_date)\n",
    "        self.arr = [None] * self.capacity\n",
    "        self.size = 0\n",
    "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.subject_embeddings = {}  # New: subject -> embedding mapping\n",
    "        self.subject_index = {}       # New: for fast similarity search\n",
    "        self._subject_embeddings_updated = False  # Track if embeddings need reindexing\n",
    "        \n",
    "\n",
    "    def date_to_value(self, date):\n",
    "        date_obj = datetime.strptime(date, \"%m/%d/%Y\")\n",
    "        year = date_obj.year\n",
    "        month = date_obj.month\n",
    "        day = date_obj.day\n",
    "        value = year * 372 + month * 31 + day\n",
    "        return value\n",
    "\n",
    "    def insert_news(self, date, triplets):\n",
    "        value = self.date_to_value(date)\n",
    "        index = value - self.start_index\n",
    "\n",
    "        if self.arr[index] is None:\n",
    "            self.arr[index] = {}  # New graph: subject -> Node\n",
    "\n",
    "        graph = self.arr[index]\n",
    "\n",
    "        for triplet in triplets:\n",
    "            subject = triplet['subject']\n",
    "            predicate = triplet['predicate']\n",
    "            object_ = triplet['object']\n",
    "\n",
    "            if subject not in graph:\n",
    "                graph[subject] = Node(subject)\n",
    "                # Add subject to embedding store if not already present\n",
    "                if subject not in self.subject_embeddings:\n",
    "                    self.subject_embeddings[subject] = self.embedder.encode(subject, convert_to_numpy=True)\n",
    "                    self._subject_embeddings_updated = True\n",
    "\n",
    "            graph[subject].add_edge(predicate, object_, self.embedder)\n",
    "\n",
    "        self.size += 1\n",
    "\n",
    "    def _build_subject_index(self):\n",
    "        \"\"\"Build a FAISS index for fast subject similarity search\"\"\"\n",
    "        if not self._subject_embeddings_updated:\n",
    "            return\n",
    "            \n",
    "        try:\n",
    "            import faiss\n",
    "        except ImportError:\n",
    "            print(\"FAISS not available, using brute-force search\")\n",
    "            self.subject_index = None\n",
    "            self._subject_embeddings_updated = False\n",
    "            return\n",
    "\n",
    "        embeddings = np.array(list(self.subject_embeddings.values())).astype('float32')\n",
    "        self.subject_index = faiss.IndexFlatIP(embeddings.shape[1])\n",
    "        self.subject_index.add(embeddings)\n",
    "        self._subject_embeddings_updated = False\n",
    "        print(\"Subject index rebuilt\")\n",
    "\n",
    "    def find_similar_subjects(self, query_subject, threshold=None):\n",
    "        \"\"\"Find all subjects similar to query_subject, sorted by similarity\"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = self.similarity_threshold\n",
    "\n",
    "        if not self.subject_embeddings:\n",
    "            return []\n",
    "\n",
    "        query_embedding = self.embedder.encode(query_subject, convert_to_numpy=True)\n",
    "        \n",
    "        # Try using FAISS if available\n",
    "        if self.subject_index is not None:\n",
    "            self._build_subject_index()\n",
    "            query_embedding = np.array([query_embedding]).astype('float32')\n",
    "            D, I = self.subject_index.search(query_embedding, len(self.subject_embeddings))\n",
    "            \n",
    "            subjects = list(self.subject_embeddings.keys())\n",
    "            results = []\n",
    "            for i, score in zip(I[0], D[0]):\n",
    "                if i == -1 or score < threshold:\n",
    "                    continue\n",
    "                results.append((subjects[i], score))\n",
    "            return sorted(results, key=lambda x: -x[1])\n",
    "        else:\n",
    "            # Fallback to brute-force search\n",
    "            results = []\n",
    "            for subject, embedding in self.subject_embeddings.items():\n",
    "                score = util.cos_sim(query_embedding, embedding).item()\n",
    "                if score >= threshold:\n",
    "                    results.append((subject, score))\n",
    "            return sorted(results, key=lambda x: -x[1])\n",
    "    \n",
    "    def search(self, date, subject_query, predicate_query,question_type, max_hops=4,threshold=None):\n",
    "        \"\"\"Search with soft matching for both subject and predicate.\n",
    "        Returns the highest-scored answer even if below threshold when no matches meet the threshold.\"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = self.similarity_threshold\n",
    "    \n",
    "        value = self.date_to_value(date)\n",
    "        index = value - self.start_index\n",
    "        graph = self.arr[index]\n",
    "        if graph is None:\n",
    "            return []\n",
    "        mytriplets = []\n",
    "        for subject, node in graph.items():\n",
    "            for predicate, objects in node.edges.items():\n",
    "                for obj in objects:\n",
    "                    mytriplets.append( { \"subject\": subject, \"predicate\": predicate, \"object\": obj })\n",
    "        if(question_type == \"Date\"):\n",
    "            mytriplets = filter_triplets_with_dates_or_numbers(mytriplets)\n",
    "        elif(question_type == \"Person\"):\n",
    "            mytriplets = filter_triplets_with_names_or_locations(mytriplets)\n",
    "\n",
    "        if question_type in [\"Date\", \"Person\"]:\n",
    "            filtered_graph = {}\n",
    "            for triplet in mytriplets:\n",
    "                subject = triplet['subject']\n",
    "                predicate = triplet['predicate']\n",
    "                obj = triplet['object']\n",
    "                \n",
    "                if subject not in filtered_graph:\n",
    "                    filtered_graph[subject] = Node(subject)\n",
    "                    if subject in self.subject_embeddings:\n",
    "                        filtered_graph[subject].predicate_embeddings = graph[subject].predicate_embeddings.copy()\n",
    "                \n",
    "                filtered_graph[subject].edges[predicate].append(obj)\n",
    "                if predicate not in filtered_graph[subject].predicate_embeddings:\n",
    "                    filtered_graph[subject].predicate_embeddings[predicate] = \\\n",
    "                        graph[subject].predicate_embeddings.get(predicate, \n",
    "                        self.embedder.encode(predicate, convert_to_numpy=True))\n",
    "        \n",
    "            graph = filtered_graph\n",
    "    \n",
    "        # Find all matching subjects (including exact match)\n",
    "        similar_subjects = self.find_similar_subjects(subject_query, threshold=0)  # Get all possible matches\n",
    "        if not similar_subjects:\n",
    "            return []\n",
    "    \n",
    "        results = []\n",
    "        query_pred_embedding = self.embedder.encode(predicate_query, convert_to_numpy=True)\n",
    "        \n",
    "        for matched_subject, subject_score in similar_subjects:\n",
    "            if matched_subject not in graph:\n",
    "                continue\n",
    "    \n",
    "            node = graph[matched_subject]\n",
    "            \n",
    "            # Check for exact predicate match first\n",
    "            if predicate_query in node.edges:\n",
    "                for obj in node.edges[predicate_query]:\n",
    "                    results.append({\n",
    "                        'subject': matched_subject,\n",
    "                        'subject_score': subject_score,\n",
    "                        'predicate': predicate_query,\n",
    "                        'predicate_score': 1.0,\n",
    "                        'object': obj,\n",
    "                        'path': None,\n",
    "                        'hops': 0,\n",
    "                        'total_score': subject_score + 1.0\n",
    "                    })\n",
    "            \n",
    "            # Perform soft matching for predicates\n",
    "            for pred, emb in node.predicate_embeddings.items():\n",
    "                pred_score = util.cos_sim(query_pred_embedding, emb).item()\n",
    "                # Only proceed if there are objects for this predicate\n",
    "                if pred in node.edges:\n",
    "                    for obj in node.edges[pred]:\n",
    "                        results.append({\n",
    "                            'subject': matched_subject,\n",
    "                            'subject_score': subject_score,\n",
    "                            'predicate': pred,\n",
    "                            'predicate_score': pred_score,\n",
    "                            'object': obj,\n",
    "                            'path': None,\n",
    "                            'hops': 0,\n",
    "                            'total_score': subject_score + pred_score\n",
    "                        })\n",
    "    \n",
    "            # Multi-hop search if enabled\n",
    "            if max_hops > 0:\n",
    "                visited = set()\n",
    "                queue = deque()\n",
    "                queue.append((node, [], 0))\n",
    "                \n",
    "                while queue:\n",
    "                    current_node, path, hop_count = queue.popleft()\n",
    "                    \n",
    "                    if current_node.subject in visited or hop_count >= max_hops:\n",
    "                        continue\n",
    "                        \n",
    "                    visited.add(current_node.subject)\n",
    "                    \n",
    "                    for pred, objects in current_node.edges.items():\n",
    "                        pred_score = util.cos_sim(query_pred_embedding, current_node.predicate_embeddings[pred]).item()\n",
    "                        \n",
    "                        for obj in objects:\n",
    "                            full_path = path + [pred]\n",
    "                            results.append({\n",
    "                                'subject': matched_subject,\n",
    "                                'subject_score': subject_score,\n",
    "                                'predicate': pred,\n",
    "                                'predicate_score': pred_score,\n",
    "                                'object': obj,\n",
    "                                'path': full_path,\n",
    "                                'hops': hop_count + 1,\n",
    "                                'total_score': subject_score + pred_score\n",
    "                            })\n",
    "                        \n",
    "                        # Add neighbors to queue\n",
    "                        for obj in objects:\n",
    "                            if obj in graph:\n",
    "                                queue.append((graph[obj], path + [pred], hop_count + 1))\n",
    "    \n",
    "        # Sort all results by total score descending\n",
    "        results.sort(key=lambda x: -x['total_score'])\n",
    "        \n",
    "        # Return results that meet the threshold if any exist\n",
    "        threshold_met_results = [r for r in results if r['subject_score'] >= threshold and r['predicate_score'] >= threshold]\n",
    "        \n",
    "        if threshold_met_results:\n",
    "            return threshold_met_results\n",
    "        elif results:  # Return top result even if below threshold\n",
    "            return [results[0]]\n",
    "        return []\n",
    "\n",
    "    def search_month(self, month, year, subject, predicate_query,question_type, max_hops=4,threshold=None):\n",
    "        \"\"\"Search across a whole month with soft matching\"\"\"\n",
    "        if threshold is None:\n",
    "            threshold = self.similarity_threshold\n",
    "\n",
    "        _, num_days = calendar.monthrange(year, month)\n",
    "        all_results = []\n",
    "\n",
    "        for day in range(1, num_days + 1):\n",
    "            date_str = f\"{month:02d}/{day:02d}/{year}\"\n",
    "            daily_results = self.search(date_str, subject, predicate_query,question_type, max_hops,threshold)\n",
    "            all_results.extend(daily_results)\n",
    "\n",
    "        return all_results\n",
    "\n",
    "\n",
    "    def print_graph(self, date):\n",
    "        value = self.date_to_value(date)\n",
    "        index = value - self.start_index\n",
    "        graph = self.arr[index]\n",
    "\n",
    "        if not graph:\n",
    "            print(\"No news for this date.\")\n",
    "            return\n",
    "\n",
    "        for subject, node in graph.items():\n",
    "            print(f\"Subject: {subject}\")\n",
    "            for predicate, objects in node.edges.items():\n",
    "                for obj in objects:\n",
    "                    print(f\"  --[{predicate}]--> {obj}\")\n",
    "\n",
    "    def print_graphs_10_days(self, start_date):\n",
    "        start_value = self.date_to_value(start_date)\n",
    "        start_index = start_value - self.start_index\n",
    "\n",
    "        print(f\"Graphs for 10 days starting from {start_date}\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        for day_offset in range(10):\n",
    "            current_index = start_index + day_offset\n",
    "            if current_index < 0 or current_index >= self.capacity:\n",
    "                continue  # out of bounds\n",
    "\n",
    "            graph = self.arr[current_index]\n",
    "\n",
    "            base_date = datetime.strptime(start_date, \"%m/%d/%Y\")\n",
    "            current_date = base_date + timedelta(days=day_offset)\n",
    "            date_str = current_date.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "            print(f\"Date: {date_str}\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "            if graph is None:\n",
    "                print(\"  No news for this date.\")\n",
    "                print(\"-\" * 60)\n",
    "                continue\n",
    "\n",
    "            for subject, node in graph.items():\n",
    "                print(f\"  Subject: {subject}\")\n",
    "                for predicate, objects in node.edges.items():\n",
    "                    for obj in objects:\n",
    "                        print(f\"    - [{predicate}] -> {obj}\")\n",
    "            print(\"-\" * 60)\n",
    "    def save(self, filepath):\n",
    "        with open(filepath, \"wb\") as f:\n",
    "            pickle.dump(self, f)\n",
    "        print(f\"Hash object saved to {filepath}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def load(filepath):\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            obj = pickle.load(f)\n",
    "        print(f\"Hash object loaded from {filepath}.\")\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you Have triplets and want to build Manual Graph then insert your data here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected DF format  \n",
    "\n",
    "\n",
    "\n",
    "Sequential_Date       |      triplets   \n",
    "\n",
    "\n",
    "\n",
    "Note We assume that the date is mm/dd/yyyy\n",
    "\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-15T22:29:13.797700Z",
     "iopub.status.busy": "2025-05-15T22:29:13.797407Z",
     "iopub.status.idle": "2025-05-15T22:30:48.704358Z",
     "shell.execute_reply": "2025-05-15T22:30:48.703454Z",
     "shell.execute_reply.started": "2025-05-15T22:29:13.797680Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "date_knowledge_graphs = {}\n",
    "bad_indexes = []\n",
    "\n",
    "for index, row in triplets_with_dates.iterrows():\n",
    "    try:\n",
    "        # 1. Convert date format\n",
    "        formatted_date = datetime.strptime(row['Sequential_Date'], '%m-%d-%Y').strftime('%m/%d/%Y')\n",
    "        \n",
    "        # 2. Process triplets\n",
    "        raw_triplets = row['triplets']\n",
    "        \n",
    "        # Convert string representation if needed\n",
    "        if isinstance(raw_triplets, str):\n",
    "            try:\n",
    "                triplets = ast.literal_eval(raw_triplets.strip())\n",
    "            except Exception as e:\n",
    "                bad_indexes.append(index)\n",
    "                print(f\"Row {index}: Parse error - {str(e)}\")\n",
    "                continue\n",
    "        else:\n",
    "            triplets = raw_triplets\n",
    "\n",
    "        # Validate triplets structure\n",
    "        is_valid = (\n",
    "            isinstance(triplets, list) and \n",
    "            all(\n",
    "                isinstance(t, dict) and \n",
    "                {'subject', 'predicate', 'object'}.issubset(t.keys())\n",
    "                for t in triplets\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        if not is_valid:\n",
    "            print(f\"Row {index}: Invalid triplets format\")\n",
    "            print(f\"Sample: {str(triplets)[:200]}\")\n",
    "            bad_indexes.append(index)\n",
    "            continue\n",
    "            \n",
    "        # Create inverted triplets\n",
    "        inverted_triplets = []\n",
    "        for triplet in triplets:\n",
    "            inverted_triplet = {\n",
    "                'subject': triplet['object'],\n",
    "                'predicate': triplet['predicate'], \n",
    "                'object': triplet['subject']\n",
    "            }\n",
    "            inverted_triplets.append(inverted_triplet)\n",
    "        \n",
    "        # Insert both original and inverted triplets\n",
    "        sohailo.insert_news(formatted_date, inverted_triplets)  # Inverted\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nCritical error in row {index}: {str(e)}\")\n",
    "        bad_indexes.append(index)\n",
    "        if 'triplets' in locals():\n",
    "            print(f\"Triplets type: {type(triplets)}\")\n",
    "            print(f\"Triplets content: {str(triplets)[:200]}...\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessfully processed {len(triplets_with_dates) - len(bad_indexes)} rows\")\n",
    "print(f\"Failed to process {len(bad_indexes)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "triplets_with_dates = triplets_with_dates.drop(index=bad_indexes)  # Removes rows 1, 3, and 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "triplets_with_dates.to_csv(\"cleaned date data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save the model into a pkl file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kg2 = Hash(\"01/01/2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "kg2.save(\"magdySquad.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you already have your Graph saved as pkl file Just Load dont Build and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:56.688603Z",
     "iopub.status.busy": "2025-05-21T13:01:56.687757Z",
     "iopub.status.idle": "2025-05-21T13:01:58.218262Z",
     "shell.execute_reply": "2025-05-21T13:01:58.217046Z",
     "shell.execute_reply.started": "2025-05-21T13:01:56.688568Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash object loaded from C:\\Users\\ADMIN\\Downloads\\finalSquad_100.pkl.\n"
     ]
    }
   ],
   "source": [
    "kg2 = Hash(\"01/01/2000\")\n",
    "Manual_Graph = kg2.load(\"C:\\\\Users\\\\ADMIN\\\\Downloads\\\\finalSquad_100.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:58.220700Z",
     "iopub.status.busy": "2025-05-21T13:01:58.220273Z",
     "iopub.status.idle": "2025-05-21T13:01:58.229062Z",
     "shell.execute_reply": "2025-05-21T13:01:58.228240Z",
     "shell.execute_reply.started": "2025-05-21T13:01:58.220671Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.Hash"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Manual_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:58.230318Z",
     "iopub.status.busy": "2025-05-21T13:01:58.230029Z",
     "iopub.status.idle": "2025-05-21T13:01:58.263175Z",
     "shell.execute_reply": "2025-05-21T13:01:58.261745Z",
     "shell.execute_reply.started": "2025-05-21T13:01:58.230290Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphs for 10 days starting from 1/4/2018\n",
      "============================================================\n",
      "Date: 01/04/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: individual and group identity\n",
      "    - [is] -> psychologists\n",
      "    - [is] -> sociologists\n",
      "    - [is] -> anthropologists\n",
      "  Subject: a structural representation of the individual's existential experience\n",
      "    - [is] -> Weinreich's Identity Structure Analysis (ISA)\n",
      "    - [is] -> Weinreich's Identity Structure Analysis\n",
      "  Subject: organised in relatively stable structures over time\n",
      "    - [are] -> the relationships\n",
      "  Subject: in which self relates to other agents and institutions\n",
      "    - [is] -> the socio-cultural milieu\n",
      "  Subject: constructs drawn from the salient discourses of the individual, the group and cultural norms\n",
      "    - [uses] -> the individual\n",
      "  Subject: a methodology that maps how these are used by the individual\n",
      "    - [provides] -> the practical operationalisation of ISA\n",
      "  Subject: self and other agents and institutions\n",
      "    - [appraises] -> the individual\n",
      "    - [appraises] -> situated self\n",
      "    - [relates] -> socio-cultural milieu\n",
      "  Subject: in the individual's evaluation of self and significant others and institutions\n",
      "    - [results] -> the individual's evaluation\n",
      "  Subject: description or representation of individual and group identity\n",
      "    - [have central task] -> psychologists\n",
      "    - [have central task] -> sociologists\n",
      "    - [have central task] -> anthropologists\n",
      "  Subject: identity to be mapped and defined\n",
      "    - [require] -> disciplines\n",
      "  Subject: idiosyncratic qualities and group memberships or identifications\n",
      "    - [encompasses] -> identity\n",
      "    - [can shift] -> circumstance\n",
      "  Subject: circumstance\n",
      "    - [can shift] -> idiosyncratic qualities and group memberships or identifications\n",
      "  Subject: socio-cultural milieu\n",
      "    - [emphasizes] -> Weinreich's Identity Structure Analysis\n",
      "    - [relates] -> self and other agents and institutions\n",
      "  Subject: relatively stable structures over time\n",
      "    - [are organised in] -> relationships between self and other agents\n",
      "  Subject: Identity Structure Analysis book\n",
      "    - [edited] -> Weinreich and Saunderson\n",
      "    - [was published] -> 2003\n",
      "  Subject: 2003\n",
      "    - [was published] -> Identity Structure Analysis book\n",
      "  Subject: constructs drawn from salient discourses of the individual, the group, and cultural norms\n",
      "    - [uses] -> Identity Structure Analysis\n",
      "  Subject: methodology that maps how constructs are used by the individual\n",
      "    - [provides] -> practical operationalisation of Identity Structure Analysis\n",
      "  Subject: across time and milieus\n",
      "    - [applies constructs] -> situated self\n",
      "  Subject: individual's evaluation of self and significant others and institutions\n",
      "    - [results in] -> appraisal by situated self\n",
      "  Subject: Identity theory\n",
      "    - [contributed to] -> Kelly\n",
      "    - [contributed to] -> Erikson\n",
      "    - [contributed to] -> Tajfel\n",
      "  Subject: Identity Structure Analysis\n",
      "    - [stands for] -> ISA\n",
      "    - [uses] -> constructs drawn from salient discourses of the individual, the group, and cultural norms\n",
      "  Subject: psychologists\n",
      "    - [is] -> individual and group identity\n",
      "    - [have central task] -> description or representation of individual and group identity\n",
      "  Subject: sociologists\n",
      "    - [is] -> individual and group identity\n",
      "    - [have central task] -> description or representation of individual and group identity\n",
      "  Subject: anthropologists\n",
      "    - [is] -> individual and group identity\n",
      "    - [have central task] -> description or representation of individual and group identity\n",
      "  Subject: Weinreich's Identity Structure Analysis (ISA)\n",
      "    - [is] -> a structural representation of the individual's existential experience\n",
      "  Subject: the relationships\n",
      "    - [are] -> organised in relatively stable structures over time\n",
      "  Subject: the socio-cultural milieu\n",
      "    - [is] -> in which self relates to other agents and institutions\n",
      "  Subject: the individual\n",
      "    - [uses] -> constructs drawn from the salient discourses of the individual, the group and cultural norms\n",
      "    - [appraises] -> self and other agents and institutions\n",
      "  Subject: the practical operationalisation of ISA\n",
      "    - [provides] -> a methodology that maps how these are used by the individual\n",
      "  Subject: the individual's evaluation\n",
      "    - [results] -> in the individual's evaluation of self and significant others and institutions\n",
      "  Subject: disciplines\n",
      "    - [require] -> identity to be mapped and defined\n",
      "  Subject: identity\n",
      "    - [encompasses] -> idiosyncratic qualities and group memberships or identifications\n",
      "  Subject: Weinreich's Identity Structure Analysis\n",
      "    - [is] -> a structural representation of the individual's existential experience\n",
      "    - [emphasizes] -> socio-cultural milieu\n",
      "  Subject: relationships between self and other agents\n",
      "    - [are organised in] -> relatively stable structures over time\n",
      "  Subject: Weinreich and Saunderson\n",
      "    - [edited] -> Identity Structure Analysis book\n",
      "  Subject: practical operationalisation of Identity Structure Analysis\n",
      "    - [provides] -> methodology that maps how constructs are used by the individual\n",
      "  Subject: situated self\n",
      "    - [applies constructs] -> across time and milieus\n",
      "    - [appraises] -> self and other agents and institutions\n",
      "  Subject: appraisal by situated self\n",
      "    - [results in] -> individual's evaluation of self and significant others and institutions\n",
      "  Subject: Kelly\n",
      "    - [contributed to] -> Identity theory\n",
      "  Subject: Erikson\n",
      "    - [contributed to] -> Identity theory\n",
      "  Subject: Tajfel\n",
      "    - [contributed to] -> Identity theory\n",
      "  Subject: ISA\n",
      "    - [stands for] -> Identity Structure Analysis\n",
      "------------------------------------------------------------\n",
      "Date: 01/05/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: 15 April 1948\n",
      "    - [came into being] -> Chief Commissioner's Province of H.P.\n",
      "    - [came into being] -> Chief Commissioner's Province of Himachal Pradesh\n",
      "  Subject: State of Bilaspur\n",
      "    - [was merged] -> Himachal Pradesh\n",
      "    - [was merged into] -> Himachal Pradesh\n",
      "    - [was merged on] -> 1 April 1954\n",
      "  Subject: part C state\n",
      "    - [became] -> Himachal\n",
      "  Subject: \n",
      "    - [was appointed] -> Lt. Governor\n",
      "  Subject: 1952\n",
      "    - [was elected] -> Legislative Assembly\n",
      "    - [was elected in] -> Legislative Assembly\n",
      "  Subject: union territory\n",
      "    - [became] -> Himachal Pradesh\n",
      "  Subject: Himachal Pradesh\n",
      "    - [were merged] -> Punjab State\n",
      "    - [was merged into] -> State of Bilaspur\n",
      "    - [was merged into] -> Nalagarh tehsil of Ambala District\n",
      "    - [was merged into] -> Santokhgarh kanungo circle (part)\n",
      "    - [was merged into] -> Una tehsil of Hoshiarpur District (part)\n",
      "    - [was merged into] -> Dhar Kalan Kanungo circle of Pathankot tehsil (part)\n",
      "    - [was appointed for] -> Lt. Governor\n",
      "    - [were merged into] -> Simla, Kangra, Kulu and Lahul and Spiti Districts\n",
      "    - [were merged into] -> Lohara, Amb and Una kanungo circles\n",
      "    - [was merged] -> State of Bilaspur\n",
      "    - [became] -> union territory\n",
      "    - [became part C state] -> 26 January 1950\n",
      "    - [became union territory] -> 1 November 1956\n",
      "    - [became new state] -> 25 January 1971\n",
      "    - [emerged as] -> 18th state of Indian Union\n",
      "  Subject: Parliament\n",
      "    - [was passed] -> State of Himachal Pradesh Act\n",
      "    - [was passed by] -> State of Himachal Pradesh Act\n",
      "  Subject: 25 January 1971\n",
      "    - [came into being] -> new state\n",
      "    - [became new state] -> Himachal Pradesh\n",
      "  Subject: integration of 28 petty princely states\n",
      "    - [was created by] -> Chief Commissioner's Province of Himachal Pradesh\n",
      "  Subject: feudal princes and zaildars\n",
      "    - [included] -> 28 petty princely states\n",
      "  Subject: Simla Hills States and four Punjab southern hill states\n",
      "    - [were located in] -> 28 petty princely states\n",
      "  Subject: Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947\n",
      "    - [was issued under] -> Himachal Pradesh (Administration) Order, 1948\n",
      "  Subject: Foreign Jurisdiction Act, 1947\n",
      "    - [was renamed] -> Extra-Provincial Jurisdiction Act, 1947\n",
      "    - [was renamed by] -> A.O. of 1950\n",
      "  Subject: A.O. of 1950\n",
      "    - [was renamed by] -> Foreign Jurisdiction Act, 1947\n",
      "  Subject: 1 April 1954\n",
      "    - [was merged on] -> State of Bilaspur\n",
      "  Subject: merger of Bilaspur with Himachal Pradesh\n",
      "    - [enabled] -> Himachal Pradesh and Bilaspur (New State) Act, 1954\n",
      "  Subject: 26 January 1950\n",
      "    - [became part C state] -> Himachal Pradesh\n",
      "    - [was implemented on] -> Constitution of India\n",
      "  Subject: 1 November 1956\n",
      "    - [became union territory] -> Himachal Pradesh\n",
      "  Subject: Parliament of India\n",
      "    - [was enacted by] -> Punjab Reorganisation Act, 1966\n",
      "  Subject: merger of Punjab areas with Himachal Pradesh\n",
      "    - [enabled] -> Punjab Reorganisation Act, 1966\n",
      "  Subject: 1 November 1966\n",
      "    - [were merged into Himachal Pradesh on] -> Punjab areas\n",
      "  Subject: 18 December 1970\n",
      "    - [was passed on] -> State of Himachal Pradesh Act\n",
      "  Subject: 18th state of Indian Union\n",
      "    - [emerged as] -> Himachal Pradesh\n",
      "  Subject: Chief Commissioner's Province of H.P.\n",
      "    - [came into being] -> 15 April 1948\n",
      "  Subject: Himachal\n",
      "    - [became] -> part C state\n",
      "  Subject: Lt. Governor\n",
      "    - [was appointed] -> \n",
      "    - [was appointed for] -> Himachal Pradesh\n",
      "  Subject: Legislative Assembly\n",
      "    - [was elected] -> 1952\n",
      "    - [was elected in] -> 1952\n",
      "  Subject: Punjab State\n",
      "    - [were merged] -> Himachal Pradesh\n",
      "  Subject: State of Himachal Pradesh Act\n",
      "    - [was passed] -> Parliament\n",
      "    - [was passed by] -> Parliament\n",
      "    - [was passed on] -> 18 December 1970\n",
      "  Subject: new state\n",
      "    - [came into being] -> 25 January 1971\n",
      "  Subject: Chief Commissioner's Province of Himachal Pradesh\n",
      "    - [came into being] -> 15 April 1948\n",
      "    - [was created by] -> integration of 28 petty princely states\n",
      "  Subject: 28 petty princely states\n",
      "    - [included] -> feudal princes and zaildars\n",
      "    - [were located in] -> Simla Hills States and four Punjab southern hill states\n",
      "  Subject: Himachal Pradesh (Administration) Order, 1948\n",
      "    - [was issued under] -> Sections 3 and 4 of the Extra-Provincial Jurisdiction Act, 1947\n",
      "  Subject: Extra-Provincial Jurisdiction Act, 1947\n",
      "    - [was renamed] -> Foreign Jurisdiction Act, 1947\n",
      "  Subject: Himachal Pradesh and Bilaspur (New State) Act, 1954\n",
      "    - [enabled] -> merger of Bilaspur with Himachal Pradesh\n",
      "  Subject: Constitution of India\n",
      "    - [was implemented on] -> 26 January 1950\n",
      "  Subject: Simla, Kangra, Kulu and Lahul and Spiti Districts\n",
      "    - [were merged into] -> Himachal Pradesh\n",
      "  Subject: Nalagarh tehsil of Ambala District\n",
      "    - [was merged into] -> Himachal Pradesh\n",
      "  Subject: Lohara, Amb and Una kanungo circles\n",
      "    - [were merged into] -> Himachal Pradesh\n",
      "  Subject: Santokhgarh kanungo circle (part)\n",
      "    - [was merged into] -> Himachal Pradesh\n",
      "  Subject: Una tehsil of Hoshiarpur District (part)\n",
      "    - [was merged into] -> Himachal Pradesh\n",
      "  Subject: Dhar Kalan Kanungo circle of Pathankot tehsil (part)\n",
      "    - [was merged into] -> Himachal Pradesh\n",
      "  Subject: Punjab Reorganisation Act, 1966\n",
      "    - [was enacted by] -> Parliament of India\n",
      "    - [enabled] -> merger of Punjab areas with Himachal Pradesh\n",
      "  Subject: Punjab areas\n",
      "    - [were merged into Himachal Pradesh on] -> 1 November 1966\n",
      "------------------------------------------------------------\n",
      "Date: 01/06/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: to coordinate and organize their growth and development\n",
      "    - [communicate] -> Fungi\n",
      "  Subject: with their own and related species\n",
      "    - [communicate] -> Fungi\n",
      "  Subject: with non fungal organisms\n",
      "    - [communicate] -> Fungi\n",
      "  Subject: the fungal organism to react\n",
      "    - [trigger] -> The biochemicals\n",
      "  Subject: between molecules taking part in biotic messages and similar molecules being irrelevant in the situation\n",
      "    - [differentiate] -> Fungal organisms\n",
      "  Subject: to coordinate different behavioral patterns\n",
      "    - [are known] -> Five different primary signalling molecules\n",
      "  Subject: through interpretation processes\n",
      "    - [is achieved] -> Behavioral coordination\n",
      "  Subject: to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out 'noise'\n",
      "    - [enables] -> The organism\n",
      "  Subject: to coordinate and organize growth and development\n",
      "    - [communicate] -> Fungi\n",
      "  Subject: formation of Marcelia and fruiting bodies\n",
      "    - [coordinate] -> Fungi\n",
      "  Subject: their own and related species\n",
      "    - [communicate with] -> Fungi\n",
      "  Subject: non-fungal organisms\n",
      "    - [communicate with] -> Fungi\n",
      "  Subject: bacteria, unicellular eukaryotes, plants, and insects\n",
      "    - [include] -> Non-fungal organisms\n",
      "  Subject: biochemicals of biotic origin\n",
      "    - [occurs through] -> Communication between fungi and other organisms\n",
      "  Subject: fungal organism to react in a specific manner\n",
      "    - [trigger] -> Biochemicals of biotic origin\n",
      "  Subject: fungal organism to react\n",
      "    - [do not trigger] -> Non-biotic chemical molecules\n",
      "  Subject: biotic messages and irrelevant molecules\n",
      "    - [can differentiate between] -> Fungal organisms\n",
      "  Subject: to coordinate behavioral patterns\n",
      "    - [are known] -> Five primary signalling molecules\n",
      "  Subject: filamentation, mating, growth, and pathogenicity\n",
      "    - [coordinate] -> Primary signalling molecules\n",
      "  Subject: interpretation processes\n",
      "    - [is achieved through] -> Behavioral coordination and production of signaling substances\n",
      "  Subject: fungi to distinguish self or non-self\n",
      "    - [enable] -> Interpretation processes\n",
      "  Subject: fungi to identify biotic indicators and messages\n",
      "    - [enable] -> Interpretation processes\n",
      "  Subject: fungi to identify messages from similar, related, or non-related species\n",
      "    - [enable] -> Interpretation processes\n",
      "  Subject: fungi to filter out noise\n",
      "    - [enable] -> Interpretation processes\n",
      "  Subject: similar molecules without biotic content\n",
      "    - [is defined as] -> Noise\n",
      "  Subject: Fungi\n",
      "    - [communicate] -> to coordinate and organize their growth and development\n",
      "    - [communicate] -> with their own and related species\n",
      "    - [communicate] -> with non fungal organisms\n",
      "    - [communicate] -> to coordinate and organize growth and development\n",
      "    - [coordinate] -> formation of Marcelia and fruiting bodies\n",
      "    - [communicate with] -> their own and related species\n",
      "    - [communicate with] -> non-fungal organisms\n",
      "  Subject: The biochemicals\n",
      "    - [trigger] -> the fungal organism to react\n",
      "  Subject: Fungal organisms\n",
      "    - [differentiate] -> between molecules taking part in biotic messages and similar molecules being irrelevant in the situation\n",
      "    - [can differentiate between] -> biotic messages and irrelevant molecules\n",
      "  Subject: Five different primary signalling molecules\n",
      "    - [are known] -> to coordinate different behavioral patterns\n",
      "  Subject: Behavioral coordination\n",
      "    - [is achieved] -> through interpretation processes\n",
      "  Subject: The organism\n",
      "    - [enables] -> to differ between self or non-self, a biotic indicator, biotic message from similar, related, or non-related species, and even filter out 'noise'\n",
      "  Subject: Non-fungal organisms\n",
      "    - [include] -> bacteria, unicellular eukaryotes, plants, and insects\n",
      "  Subject: Communication between fungi and other organisms\n",
      "    - [occurs through] -> biochemicals of biotic origin\n",
      "  Subject: Biochemicals of biotic origin\n",
      "    - [trigger] -> fungal organism to react in a specific manner\n",
      "  Subject: Non-biotic chemical molecules\n",
      "    - [do not trigger] -> fungal organism to react\n",
      "  Subject: Five primary signalling molecules\n",
      "    - [are known] -> to coordinate behavioral patterns\n",
      "  Subject: Primary signalling molecules\n",
      "    - [coordinate] -> filamentation, mating, growth, and pathogenicity\n",
      "  Subject: Behavioral coordination and production of signaling substances\n",
      "    - [is achieved through] -> interpretation processes\n",
      "  Subject: Interpretation processes\n",
      "    - [enable] -> fungi to distinguish self or non-self\n",
      "    - [enable] -> fungi to identify biotic indicators and messages\n",
      "    - [enable] -> fungi to identify messages from similar, related, or non-related species\n",
      "    - [enable] -> fungi to filter out noise\n",
      "  Subject: Noise\n",
      "    - [is defined as] -> similar molecules without biotic content\n",
      "------------------------------------------------------------\n",
      "Date: 01/07/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: 6,0008,000 years ago in the Near East\n",
      "    - [began] -> The cultivation of the domesticated grape\n",
      "  Subject: naturally on the skins of grapes\n",
      "    - [occurs] -> Yeast\n",
      "  Subject: from 8,000 years ago in Georgia\n",
      "    - [dates] -> The earliest archeological evidence\n",
      "  Subject: in Armenia, dating to around 4000 BC\n",
      "    - [was found] -> The oldest winery\n",
      "  Subject: to produce some of the finest wines in the Middle East\n",
      "    - [was known] -> The city of Shiraz\n",
      "  Subject: that Syrah red wine is named after\n",
      "    - [has been proposed] -> Shiraz\n",
      "  Subject: the cultivation of purple grapes\n",
      "    - [record] -> Ancient Egyptian hieroglyphics\n",
      "  Subject: to the ancient Greeks, Phoenicians, and Romans growing purple grapes for both eating and wine production\n",
      "    - [attests] -> History\n",
      "  Subject: to other regions in Europe, as well as North Africa, and eventually in North America\n",
      "    - [would later spread] -> The growing of grapes\n",
      "  Subject: 6,0008,000 years ago\n",
      "    - [began] -> Cultivation of the domesticated grape\n",
      "  Subject: the Near East\n",
      "    - [occurred in] -> Cultivation of the domesticated grape\n",
      "  Subject: skins of grapes\n",
      "    - [occurs naturally on] -> Yeast\n",
      "  Subject: one of the earliest domesticated microorganisms\n",
      "    - [is] -> Yeast\n",
      "  Subject: alcoholic drinks such as wine\n",
      "    - [led to innovation of] -> Yeast\n",
      "  Subject: 8,000 years ago\n",
      "    - [dates from] -> Earliest archaeological evidence for wine-making\n",
      "  Subject: Georgia\n",
      "    - [was found in] -> Earliest archaeological evidence for wine-making\n",
      "  Subject: Armenia\n",
      "    - [was found in] -> Oldest winery\n",
      "  Subject: around 4000 BC\n",
      "    - [dates to] -> Oldest winery\n",
      "  Subject: finest wines in the Middle East\n",
      "    - [was known to produce] -> City of Shiraz\n",
      "  Subject: 9th century AD\n",
      "    - [was known for wine production by] -> City of Shiraz\n",
      "  Subject: Shiraz, a city in Persia\n",
      "    - [is proposed to be named after] -> Syrah red wine\n",
      "  Subject: Shirazi wine\n",
      "    - [were used to make] -> Grapes used in Shiraz\n",
      "  Subject: cultivation of purple grapes\n",
      "    - [record] -> Ancient Egyptian hieroglyphics\n",
      "  Subject: purple grapes\n",
      "    - [grew] -> Ancient Greeks\n",
      "    - [grew] -> Phoenicians\n",
      "    - [grew] -> Romans\n",
      "  Subject: eating and wine production\n",
      "    - [were used for] -> Purple grapes\n",
      "  Subject: other regions in Europe\n",
      "    - [spread to] -> Grape growing\n",
      "  Subject: North Africa\n",
      "    - [spread to] -> Grape growing\n",
      "  Subject: North America\n",
      "    - [eventually spread to] -> Grape growing\n",
      "  Subject: The cultivation of the domesticated grape\n",
      "    - [began] -> 6,0008,000 years ago in the Near East\n",
      "  Subject: Yeast\n",
      "    - [occurs] -> naturally on the skins of grapes\n",
      "    - [occurs naturally on] -> skins of grapes\n",
      "    - [is] -> one of the earliest domesticated microorganisms\n",
      "    - [led to innovation of] -> alcoholic drinks such as wine\n",
      "  Subject: The earliest archeological evidence\n",
      "    - [dates] -> from 8,000 years ago in Georgia\n",
      "  Subject: The oldest winery\n",
      "    - [was found] -> in Armenia, dating to around 4000 BC\n",
      "  Subject: The city of Shiraz\n",
      "    - [was known] -> to produce some of the finest wines in the Middle East\n",
      "  Subject: Shiraz\n",
      "    - [has been proposed] -> that Syrah red wine is named after\n",
      "  Subject: Ancient Egyptian hieroglyphics\n",
      "    - [record] -> the cultivation of purple grapes\n",
      "    - [record] -> cultivation of purple grapes\n",
      "  Subject: History\n",
      "    - [attests] -> to the ancient Greeks, Phoenicians, and Romans growing purple grapes for both eating and wine production\n",
      "  Subject: The growing of grapes\n",
      "    - [would later spread] -> to other regions in Europe, as well as North Africa, and eventually in North America\n",
      "  Subject: Cultivation of the domesticated grape\n",
      "    - [began] -> 6,0008,000 years ago\n",
      "    - [occurred in] -> the Near East\n",
      "  Subject: Earliest archaeological evidence for wine-making\n",
      "    - [dates from] -> 8,000 years ago\n",
      "    - [was found in] -> Georgia\n",
      "  Subject: Oldest winery\n",
      "    - [was found in] -> Armenia\n",
      "    - [dates to] -> around 4000 BC\n",
      "  Subject: City of Shiraz\n",
      "    - [was known to produce] -> finest wines in the Middle East\n",
      "    - [was known for wine production by] -> 9th century AD\n",
      "  Subject: Syrah red wine\n",
      "    - [is proposed to be named after] -> Shiraz, a city in Persia\n",
      "  Subject: Grapes used in Shiraz\n",
      "    - [were used to make] -> Shirazi wine\n",
      "  Subject: Ancient Greeks\n",
      "    - [grew] -> purple grapes\n",
      "  Subject: Phoenicians\n",
      "    - [grew] -> purple grapes\n",
      "  Subject: Romans\n",
      "    - [grew] -> purple grapes\n",
      "  Subject: Purple grapes\n",
      "    - [were used for] -> eating and wine production\n",
      "  Subject: Grape growing\n",
      "    - [spread to] -> other regions in Europe\n",
      "    - [spread to] -> North Africa\n",
      "    - [eventually spread to] -> North America\n",
      "------------------------------------------------------------\n",
      "Date: 01/08/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: users\n",
      "    - [are designed to] -> Denial of service attacks\n",
      "  Subject: service\n",
      "    - [can deny] -> Attackers\n",
      "  Subject: individual victims\n",
      "    - [deny] -> Attackers\n",
      "    - [can deny service to] -> Attackers\n",
      "  Subject: the capabilities of a machine or network\n",
      "    - [overload] -> Attackers\n",
      "  Subject: all users at once\n",
      "    - [block] -> Attackers\n",
      "    - [blocks] -> Overloading a machine or network\n",
      "  Subject: by adding a new firewall rule\n",
      "    - [can be blocked] -> a network attack\n",
      "  Subject: possible\n",
      "    - [are] -> many forms of Distributed denial of service (DDoS) attacks\n",
      "    - [are] -> a range of other techniques\n",
      "  Subject: from the zombie computers of a botnet\n",
      "    - [can originate] -> Such attacks\n",
      "  Subject: the victim\n",
      "    - [are fooled] -> innocent systems\n",
      "  Subject: make a machine or network resource unavailable to its intended users\n",
      "    - [are designed to] -> Denial of service attacks\n",
      "  Subject: deliberately entering a wrong password enough consecutive times\n",
      "    - [can deny service by] -> Attackers\n",
      "  Subject: victim account to be locked\n",
      "    - [cause] -> Wrong password attempts\n",
      "  Subject: overloading capabilities of a machine or network\n",
      "    - [can deny service by] -> Attackers\n",
      "  Subject: adding a new firewall rule\n",
      "    - [can be blocked by] -> Network attack from a single IP address\n",
      "  Subject: network attacks from a large number of points\n",
      "    - [are] -> Distributed denial of service (DDoS) attacks\n",
      "  Subject: much more difficult\n",
      "    - [is] -> Defending against Distributed denial of service (DDoS) attacks\n",
      "  Subject: zombie computers of a botnet\n",
      "    - [can originate from] -> Distributed denial of service (DDoS) attacks\n",
      "  Subject: reflection attacks\n",
      "    - [can use] -> Distributed denial of service (DDoS) attacks\n",
      "  Subject: amplification attacks\n",
      "    - [can use] -> Distributed denial of service (DDoS) attacks\n",
      "  Subject: innocent systems into sending traffic to the victim\n",
      "    - [fool] -> Reflection and amplification attacks\n",
      "  Subject: send traffic to the victim\n",
      "    - [are used to] -> Innocent systems\n",
      "  Subject: Distributed denial of service\n",
      "    - [stands for] -> DDos\n",
      "  Subject: Denial of service attacks\n",
      "    - [are designed to] -> users\n",
      "    - [are designed to] -> make a machine or network resource unavailable to its intended users\n",
      "  Subject: Attackers\n",
      "    - [can deny] -> service\n",
      "    - [deny] -> individual victims\n",
      "    - [overload] -> the capabilities of a machine or network\n",
      "    - [block] -> all users at once\n",
      "    - [can deny service to] -> individual victims\n",
      "    - [can deny service by] -> deliberately entering a wrong password enough consecutive times\n",
      "    - [can deny service by] -> overloading capabilities of a machine or network\n",
      "  Subject: a network attack\n",
      "    - [can be blocked] -> by adding a new firewall rule\n",
      "  Subject: many forms of Distributed denial of service (DDoS) attacks\n",
      "    - [are] -> possible\n",
      "  Subject: Such attacks\n",
      "    - [can originate] -> from the zombie computers of a botnet\n",
      "  Subject: a range of other techniques\n",
      "    - [are] -> possible\n",
      "  Subject: innocent systems\n",
      "    - [are fooled] -> the victim\n",
      "  Subject: Wrong password attempts\n",
      "    - [cause] -> victim account to be locked\n",
      "  Subject: Overloading a machine or network\n",
      "    - [blocks] -> all users at once\n",
      "  Subject: Network attack from a single IP address\n",
      "    - [can be blocked by] -> adding a new firewall rule\n",
      "  Subject: Distributed denial of service (DDoS) attacks\n",
      "    - [are] -> network attacks from a large number of points\n",
      "    - [can originate from] -> zombie computers of a botnet\n",
      "    - [can use] -> reflection attacks\n",
      "    - [can use] -> amplification attacks\n",
      "  Subject: Defending against Distributed denial of service (DDoS) attacks\n",
      "    - [is] -> much more difficult\n",
      "  Subject: Reflection and amplification attacks\n",
      "    - [fool] -> innocent systems into sending traffic to the victim\n",
      "  Subject: Innocent systems\n",
      "    - [are used to] -> send traffic to the victim\n",
      "  Subject: DDos\n",
      "    - [stands for] -> Distributed denial of service\n",
      "------------------------------------------------------------\n",
      "Date: 01/09/2018\n",
      "------------------------------------------------------------\n",
      "  No news for this date.\n",
      "------------------------------------------------------------\n",
      "Date: 01/10/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: eukaryotic cells\n",
      "    - [have] -> all animals\n",
      "    - [have] -> All animals\n",
      "    - [are] -> surrounded by a characteristic extracellular matrix\n",
      "  Subject: surrounded by a characteristic extracellular matrix\n",
      "    - [are] -> eukaryotic cells\n",
      "  Subject: calcified\n",
      "    - [may be] -> this\n",
      "  Subject: structures like shells, bones, and spicules\n",
      "    - [forms] -> this\n",
      "  Subject: a relatively flexible framework\n",
      "    - [forms] -> it\n",
      "    - [forms during development] -> Extracellular matrix\n",
      "  Subject: about and be reorganized\n",
      "    - [can move] -> cells\n",
      "  Subject: made possible\n",
      "    - [are] -> complex structures\n",
      "  Subject: cells held in place by cell walls\n",
      "    - [have] -> other multicellular organisms\n",
      "    - [have] -> Plants\n",
      "    - [have] -> Fungi\n",
      "  Subject: the following intercellular junctions\n",
      "    - [are] -> unique to animal cells\n",
      "    - [include] -> tight junctions, gap junctions, and desmosomes\n",
      "  Subject: tight junctions, gap junctions, and desmosomes\n",
      "    - [include] -> the following intercellular junctions\n",
      "  Subject: characteristic extracellular matrix\n",
      "    - [are surrounded by] -> Eukaryotic cells of animals\n",
      "  Subject: collagen and elastic glycoproteins\n",
      "    - [is composed of] -> Extracellular matrix\n",
      "  Subject: shells\n",
      "    - [may be calcified to form] -> Extracellular matrix\n",
      "  Subject: bones\n",
      "    - [may be calcified to form] -> Extracellular matrix\n",
      "  Subject: spicules\n",
      "    - [may be calcified to form] -> Extracellular matrix\n",
      "  Subject: cells to move about and be reorganized\n",
      "    - [allows] -> Flexible framework\n",
      "  Subject: complex structures\n",
      "    - [makes possible] -> Cell movement and reorganization\n",
      "    - [are] -> made possible\n",
      "  Subject: progressive growth\n",
      "    - [develop by] -> Plants and fungi\n",
      "  Subject: tight junctions\n",
      "    - [have] -> Animal cells\n",
      "  Subject: gap junctions\n",
      "    - [have] -> Animal cells\n",
      "  Subject: desmosomes\n",
      "    - [have] -> Animal cells\n",
      "  Subject: intercellular junctions unique to animal cells\n",
      "    - [are] -> Tight junctions, gap junctions, and desmosomes\n",
      "  Subject: all animals\n",
      "    - [have] -> eukaryotic cells\n",
      "  Subject: this\n",
      "    - [may be] -> calcified\n",
      "    - [forms] -> structures like shells, bones, and spicules\n",
      "  Subject: it\n",
      "    - [forms] -> a relatively flexible framework\n",
      "  Subject: cells\n",
      "    - [can move] -> about and be reorganized\n",
      "  Subject: other multicellular organisms\n",
      "    - [have] -> cells held in place by cell walls\n",
      "  Subject: unique to animal cells\n",
      "    - [are] -> the following intercellular junctions\n",
      "  Subject: All animals\n",
      "    - [have] -> eukaryotic cells\n",
      "  Subject: Eukaryotic cells of animals\n",
      "    - [are surrounded by] -> characteristic extracellular matrix\n",
      "  Subject: Extracellular matrix\n",
      "    - [is composed of] -> collagen and elastic glycoproteins\n",
      "    - [may be calcified to form] -> shells\n",
      "    - [may be calcified to form] -> bones\n",
      "    - [may be calcified to form] -> spicules\n",
      "    - [forms during development] -> a relatively flexible framework\n",
      "  Subject: Flexible framework\n",
      "    - [allows] -> cells to move about and be reorganized\n",
      "  Subject: Cell movement and reorganization\n",
      "    - [makes possible] -> complex structures\n",
      "  Subject: Plants\n",
      "    - [have] -> cells held in place by cell walls\n",
      "  Subject: Fungi\n",
      "    - [have] -> cells held in place by cell walls\n",
      "  Subject: Plants and fungi\n",
      "    - [develop by] -> progressive growth\n",
      "  Subject: Animal cells\n",
      "    - [have] -> tight junctions\n",
      "    - [have] -> gap junctions\n",
      "    - [have] -> desmosomes\n",
      "  Subject: Tight junctions, gap junctions, and desmosomes\n",
      "    - [are] -> intercellular junctions unique to animal cells\n",
      "------------------------------------------------------------\n",
      "Date: 01/11/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: cooled\n",
      "    - [is] -> the hopped wort\n",
      "  Subject: a hopback\n",
      "    - [may pass] -> the hopped wort\n",
      "    - [is] -> a small vat filled with hops\n",
      "    - [acts] -> as a filter\n",
      "  Subject: a small vat filled with hops\n",
      "    - [is] -> a hopback\n",
      "  Subject: as a filter\n",
      "    - [acts] -> a hopback\n",
      "  Subject: cooled for the fermenter\n",
      "    - [is] -> the hopped wort\n",
      "  Subject: added\n",
      "    - [is] -> the yeast\n",
      "  Subject: beer\n",
      "    - [becomes] -> the wort\n",
      "  Subject: a week to months\n",
      "    - [requires] -> fermentation\n",
      "    - [requires] -> Fermentation process\n",
      "  Subject: ethanol\n",
      "    - [produces] -> yeast\n",
      "    - [produces] -> Fermentation\n",
      "  Subject: during fermentation\n",
      "    - [settles] -> fine particulate matter\n",
      "  Subject: once fermentation is complete\n",
      "    - [settles] -> the yeast\n",
      "  Subject: clear\n",
      "    - [the beer] -> the yeast\n",
      "  Subject: cooled after boiling\n",
      "    - [is] -> Hopped wort\n",
      "  Subject: yeast\n",
      "    - [is ready for] -> Hopped wort\n",
      "    - [receives] -> Fermenter\n",
      "    - [produces] -> ethanol\n",
      "  Subject: hopback\n",
      "    - [use] -> Some breweries\n",
      "  Subject: small vat filled with hops\n",
      "    - [is] -> Hopback\n",
      "  Subject: aromatic hop flavouring\n",
      "    - [adds] -> Hopback\n",
      "  Subject: filter\n",
      "    - [acts as] -> Hopback\n",
      "  Subject: cooled for fermenter\n",
      "    - [hopped wort is] -> Usually\n",
      "  Subject: wort into beer\n",
      "    - [transforms] -> Fermentation\n",
      "  Subject: type of yeast\n",
      "    - [depends on] -> Fermentation duration\n",
      "  Subject: strength of the beer\n",
      "    - [depends on] -> Fermentation duration\n",
      "  Subject: fine particulate matter to settle\n",
      "    - [causes] -> Fermentation\n",
      "  Subject: wort\n",
      "    - [is suspended in] -> Fine particulate matter\n",
      "  Subject: after fermentation is complete\n",
      "    - [settles] -> Yeast\n",
      "  Subject: beer clear\n",
      "    - [leaves] -> Settling of yeast\n",
      "  Subject: the hopped wort\n",
      "    - [is] -> cooled\n",
      "    - [is] -> cooled for the fermenter\n",
      "    - [may pass] -> a hopback\n",
      "  Subject: the yeast\n",
      "    - [is] -> added\n",
      "    - [settles] -> once fermentation is complete\n",
      "    - [the beer] -> clear\n",
      "  Subject: the wort\n",
      "    - [becomes] -> beer\n",
      "  Subject: fermentation\n",
      "    - [requires] -> a week to months\n",
      "  Subject: fine particulate matter\n",
      "    - [settles] -> during fermentation\n",
      "  Subject: Hopped wort\n",
      "    - [is] -> cooled after boiling\n",
      "    - [is ready for] -> yeast\n",
      "  Subject: Some breweries\n",
      "    - [use] -> hopback\n",
      "  Subject: Hopback\n",
      "    - [is] -> small vat filled with hops\n",
      "    - [adds] -> aromatic hop flavouring\n",
      "    - [acts as] -> filter\n",
      "  Subject: Usually\n",
      "    - [hopped wort is] -> cooled for fermenter\n",
      "  Subject: Fermenter\n",
      "    - [receives] -> yeast\n",
      "  Subject: Fermentation\n",
      "    - [transforms] -> wort into beer\n",
      "    - [produces] -> ethanol\n",
      "    - [causes] -> fine particulate matter to settle\n",
      "  Subject: Fermentation process\n",
      "    - [requires] -> a week to months\n",
      "  Subject: Fermentation duration\n",
      "    - [depends on] -> type of yeast\n",
      "    - [depends on] -> strength of the beer\n",
      "  Subject: Fine particulate matter\n",
      "    - [is suspended in] -> wort\n",
      "  Subject: Yeast\n",
      "    - [settles] -> after fermentation is complete\n",
      "  Subject: Settling of yeast\n",
      "    - [leaves] -> beer clear\n",
      "------------------------------------------------------------\n",
      "Date: 01/12/2018\n",
      "------------------------------------------------------------\n",
      "  Subject: differently\n",
      "    - [was asked] -> Race\n",
      "  Subject: the option\n",
      "    - [were given] -> respondents\n",
      "  Subject: that nearly seven million Americans identified as members of two or more races\n",
      "    - [show] -> Data\n",
      "  Subject: not directly comparable with data from the 1990 census or earlier censuses\n",
      "    - [are] -> these changes\n",
      "  Subject: recommended\n",
      "    - [is] -> Use\n",
      "    - [is] -> caution\n",
      "  Subject: in the racial composition of the US population over time\n",
      "    - [are] -> changes\n",
      "  Subject: the Census 2000\n",
      "    - [was asked differently in] -> Race\n",
      "    - [asked about race differently than] -> previous censuses\n",
      "  Subject: previous censuses\n",
      "    - [asked about race differently than] -> the Census 2000\n",
      "  Subject: selecting one or more race categories\n",
      "    - [were given the option of] -> respondents\n",
      "  Subject: racial identities\n",
      "    - [indicate] -> one or more race categories\n",
      "  Subject: nearly seven million Americans identified as members of two or more races\n",
      "    - [show that] -> Data\n",
      "  Subject: members of two or more races\n",
      "    - [identified as] -> nearly seven million Americans\n",
      "  Subject: data from the 1990 census or earlier censuses\n",
      "    - [are not directly comparable with] -> the Census 2000 data on race\n",
      "  Subject: racial composition over time\n",
      "    - [experienced changes in] -> United States population\n",
      "  Subject: changes in the racial composition of the United States population over time\n",
      "    - [is recommended when interpreting] -> Use of caution\n",
      "  Subject: United States\n",
      "    - [stands for] -> US\n",
      "  Subject: Race\n",
      "    - [was asked] -> differently\n",
      "    - [was asked differently in] -> the Census 2000\n",
      "  Subject: respondents\n",
      "    - [were given] -> the option\n",
      "    - [were given the option of] -> selecting one or more race categories\n",
      "  Subject: Data\n",
      "    - [show] -> that nearly seven million Americans identified as members of two or more races\n",
      "    - [show that] -> nearly seven million Americans identified as members of two or more races\n",
      "  Subject: these changes\n",
      "    - [are] -> not directly comparable with data from the 1990 census or earlier censuses\n",
      "  Subject: Use\n",
      "    - [is] -> recommended\n",
      "  Subject: caution\n",
      "    - [is] -> recommended\n",
      "  Subject: changes\n",
      "    - [are] -> in the racial composition of the US population over time\n",
      "  Subject: one or more race categories\n",
      "    - [indicate] -> racial identities\n",
      "  Subject: nearly seven million Americans\n",
      "    - [identified as] -> members of two or more races\n",
      "  Subject: the Census 2000 data on race\n",
      "    - [are not directly comparable with] -> data from the 1990 census or earlier censuses\n",
      "  Subject: United States population\n",
      "    - [experienced changes in] -> racial composition over time\n",
      "  Subject: Use of caution\n",
      "    - [is recommended when interpreting] -> changes in the racial composition of the United States population over time\n",
      "  Subject: US\n",
      "    - [stands for] -> United States\n",
      "------------------------------------------------------------\n",
      "Date: 01/13/2018\n",
      "------------------------------------------------------------\n",
      "  No news for this date.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "Manual_Graph.print_graphs_10_days('1/4/2018')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need some sort of normalizing to the dates e.g. Jan 1 2025 to 1/1/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:59.291601Z",
     "iopub.status.busy": "2025-05-21T13:01:59.290178Z",
     "iopub.status.idle": "2025-05-21T13:01:59.301796Z",
     "shell.execute_reply": "2025-05-21T13:01:59.300519Z",
     "shell.execute_reply.started": "2025-05-21T13:01:59.291549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def normalize_date(date_text):\n",
    "    try:\n",
    "\n",
    "        original_text_for_output = date_text.strip()\n",
    "        cleaned = re.sub(r'(\\d+)(st|nd|rd|th)', r'\\1', date_text.strip())        \n",
    "        dt = None\n",
    "        try:\n",
    "            dt = parser.parse(cleaned, fuzzy=True, dayfirst=False, default=datetime(1900, 1, 1))\n",
    "        except Exception: # General fallback, though primary parsing should handle most fuzzy cases\n",
    "            dt = parser.parse(cleaned, fuzzy=True, default=datetime(1900, 1, 1)) # Default parsing\n",
    "        \n",
    "        if re.fullmatch(r'\\d{4}', cleaned): # Use fullmatch for year-only\n",
    "            return {\"normalized_date\": f\"01/01/{cleaned}\", \"type\": 3, \"original\": original_text_for_output}\n",
    "        elif re.fullmatch(r'(?:January|February|March|April|May|June|July|August|September|October|November|December)[a-z]*\\s+\\d{4}', cleaned, re.IGNORECASE):\n",
    "            month_year = parser.parse(cleaned, default=datetime(1900,1,1)) # Re-parse for month_year object for safety\n",
    "            return {\"normalized_date\": month_year.strftime(\"%m/01/%Y\"), \"type\": 1, \"original\": original_text_for_output}\n",
    "        else:\n",
    "            return {\"normalized_date\": dt.strftime(\"%m/%d/%Y\"), \"type\": 0, \"original\": original_text_for_output}\n",
    "    except Exception as e:\n",
    "        # print(f\"Failed to parse date '{date_text}': {str(e)}\") # You can uncomment this for debugging\n",
    "        # Return None if any unhandled exception occurs during parsing or formatting\n",
    "        return {\"normalized_date\": None, \"type\": 5, \"original\": date_text, \"error\": str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:59.438993Z",
     "iopub.status.busy": "2025-05-21T13:01:59.437029Z",
     "iopub.status.idle": "2025-05-21T13:01:59.450108Z",
     "shell.execute_reply": "2025-05-21T13:01:59.448243Z",
     "shell.execute_reply.started": "2025-05-21T13:01:59.438922Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalize_date function (outputting MM/DD/YYYY for full dates):\n",
      "Original: '2023' -> Normalized: {'normalized_date': '01/01/2023', 'type': 3, 'original': '2023'}\n",
      "Original: 'March 2022' -> Normalized: {'normalized_date': '03/01/2022', 'type': 1, 'original': 'March 2022'}\n",
      "Original: '10th January 2021' -> Normalized: {'normalized_date': '01/10/2021', 'type': 0, 'original': '10th January 2021'}\n",
      "Original: '05/15/2020' -> Normalized: {'normalized_date': '05/15/2020', 'type': 0, 'original': '05/15/2020'}\n",
      "Original: '15/05/2020' -> Normalized: {'normalized_date': '05/15/2020', 'type': 0, 'original': '15/05/2020'}\n",
      "Original: '2nd Feb 2019' -> Normalized: {'normalized_date': '02/02/2019', 'type': 0, 'original': '2nd Feb 2019'}\n",
      "Original: 'July 4 2000' -> Normalized: {'normalized_date': '07/04/2000', 'type': 0, 'original': 'July 4 2000'}\n",
      "Original: 'Invalid Date' -> Normalized: {'normalized_date': None, 'type': 5, 'original': 'Invalid Date', 'error': 'String does not contain a date: Invalid Date'}\n",
      "Original: 'Tomorrow' -> Normalized: {'normalized_date': None, 'type': 5, 'original': 'Tomorrow', 'error': 'String does not contain a date: Tomorrow'}\n",
      "Original: '1999-12-31' -> Normalized: {'normalized_date': '12/31/1999', 'type': 0, 'original': '1999-12-31'}\n",
      "Original: '01-02-2023' -> Normalized: {'normalized_date': '01/02/2023', 'type': 0, 'original': '01-02-2023'}\n",
      "\n",
      "--- Specific Test for DD/MM/YYYY style input with dayfirst=False (default) ---\n",
      "Original: '02/03/2024' -> Normalized: {'normalized_date': '02/03/2024', 'type': 0, 'original': '02/03/2024'}\n",
      "Original: '14/03/2024' -> Normalized: {'normalized_date': '03/14/2024', 'type': 0, 'original': '14/03/2024'}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    test_dates = [\n",
    "        \"2023\",                            # Year only\n",
    "        \"March 2022\",                      # Month Year\n",
    "        \"10th January 2021\",               # Full date (DMY-like input)\n",
    "        \"05/15/2020\",                      # Full date (MM/DD/YYYY input)\n",
    "        \"15/05/2020\",                      # Full date (DD/MM/YYYY input, dayfirst=False will parse as May 15th)\n",
    "        \"2nd Feb 2019\",                    # Full date\n",
    "        \"July 4 2000\",                     # Full date\n",
    "        \"Invalid Date\",                    # Invalid\n",
    "        \"Tomorrow\",                        # Fuzzy, dateutil might parse this if not specific enough\n",
    "        \"1999-12-31\",                      # ISO format\n",
    "        \"01-02-2023\"                       # Ambiguous without dayfirst hint (parsed as Jan 2nd by default)\n",
    "    ]\n",
    "\n",
    "    print(\"Testing normalize_date function (outputting MM/DD/YYYY for full dates):\")\n",
    "    for date_str in test_dates:\n",
    "        normalized = normalize_date(date_str)\n",
    "        print(f\"Original: '{date_str}' -> Normalized: {normalized}\")\n",
    "\n",
    "    print(\"\\n--- Specific Test for DD/MM/YYYY style input with dayfirst=False (default) ---\")\n",
    "    # With dayfirst=False (default for parser.parse), \"01/02/YYYY\" is January 2nd.\n",
    "    # \"13/01/YYYY\" would be January 13th (unambiguous).\n",
    "    ambiguous_date_1 = \"02/03/2024\" # Expected: 02/03/2024 (Feb 3rd) because dayfirst=False\n",
    "    ambiguous_date_2 = \"14/03/2024\" # Expected: 03/14/2024 (March 14th)\n",
    "    \n",
    "    print(f\"Original: '{ambiguous_date_1}' -> Normalized: {normalize_date(ambiguous_date_1)}\")\n",
    "    print(f\"Original: '{ambiguous_date_2}' -> Normalized: {normalize_date(ambiguous_date_2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:24:19.844083Z",
     "iopub.status.busy": "2025-05-21T13:24:19.843579Z",
     "iopub.status.idle": "2025-05-21T13:24:19.861164Z",
     "shell.execute_reply": "2025-05-21T13:24:19.859923Z",
     "shell.execute_reply.started": "2025-05-21T13:24:19.844055Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subject': 'jeering of Royal March', 'predicate': 'occur', 'object': '14 June 1925'}\n",
      "{'subject': 'ground', 'predicate': 'be closed for', 'object': 'six months'}\n",
      "{'subject': 'claim', 'predicate': 'occur', 'object': '1926'}\n",
      "{'subject': 'testimonial match', 'predicate': 'occur', 'object': '3 July 1927'}\n",
      "{'subject': 'victory', 'predicate': 'occur', 'object': '1928'}\n",
      "{'subject': 'Rafael Alberti', 'predicate': 'be', 'object': \"member of Generation of '27\"}\n",
      "{'subject': 'win', 'predicate': 'occur', 'object': '23 June 1929'}\n",
      "{'subject': 'suicide', 'predicate': 'occur', 'object': '30 July 1930'}\n"
     ]
    }
   ],
   "source": [
    "# Basic regex for detecting dates, numbers, and number words\n",
    "DATE_REGEX = r\"\\b(\\d{1,2}[ \\-/])?(January|February|March|April|May|June|July|August|September|October|November|December)[ \\-/]\\d{2,4}\\b|\\b\\d{4}\\b|\\b\\d{1,4}[ ]?B\\.?C\\.?\\b\"\n",
    "NUMERIC_REGEX = r\"\\b\\d+\\b|\\bmillion\\b|\\bbillion\\b|\\bthousand\\b|\\bhundred\\b|\\bfew\\b|\\bseveral\\b|\\bone\\b|\\btwo\\b|\\bthree\\b|\\bfour\\b|\\bfive\\b|\\bsix\\b|\\bseven\\b|\\beight\\b|\\bnine\\b|\\bten\\b\"\n",
    "\n",
    "# Combine both into one for checking subject or object\n",
    "COMBINED_REGEX = f\"({DATE_REGEX})|({NUMERIC_REGEX})\"\n",
    "\n",
    "def filter_triplets_with_dates_or_numbers(triplets):\n",
    "    filtered = []\n",
    "    for triplet in triplets:\n",
    "        subj = triplet[\"subject\"].lower()\n",
    "        obj = triplet[\"object\"].lower()\n",
    "        if re.search(COMBINED_REGEX, subj) or re.search(COMBINED_REGEX, obj):\n",
    "            filtered.append(triplet)\n",
    "    return filtered\n",
    "triplets = [\n",
    "  {\"subject\": \"crowd\", \"predicate\": \"jeer\", \"object\": \"Royal March\"},\n",
    "  {\"subject\": \"jeering of Royal March\", \"predicate\": \"occur\", \"object\": \"14 June 1925\"},\n",
    "  {\"subject\": \"jeering of Royal March\", \"predicate\": \"be\", \"object\": \"spontaneous reaction against Primo de Rivera's dictatorship\"},\n",
    "  {\"subject\": \"ground\", \"predicate\": \"be closed for\", \"object\": \"six months\"},\n",
    "  {\"subject\": \"Gamper\", \"predicate\": \"relinquish\", \"object\": \"presidency of the club\"},\n",
    "  {\"subject\": \"event\", \"predicate\": \"coincide with\", \"object\": \"transition to professional football\"},\n",
    "  {\"subject\": \"directors of Barcelona\", \"predicate\": \"claim\", \"object\": \"to operate a professional football club\"},\n",
    "  {\"subject\": \"claim\", \"predicate\": \"occur\", \"object\": \"1926\"},\n",
    "  {\"subject\": \"club\", \"predicate\": \"hold\", \"object\": \"testimonial match for Paulino Alcntara\"},\n",
    "  {\"subject\": \"testimonial match\", \"predicate\": \"occur\", \"object\": \"3 July 1927\"},\n",
    "  {\"subject\": \"opponent in testimonial match\", \"predicate\": \"be\", \"object\": \"Spanish national team\"},\n",
    "  {\"subject\": \"Josep Canudas\", \"predicate\": \"drop\", \"object\": \"ball onto the pitch\"},\n",
    "  {\"subject\": \"Josep Canudas\", \"predicate\": \"be\", \"object\": \"local journalist and pilot\"},\n",
    "  {\"subject\": \"ball drop\", \"predicate\": \"occur from\", \"object\": \"airplane\"},\n",
    "  {\"subject\": \"victory\", \"predicate\": \"occur\", \"object\": \"1928\"},\n",
    "  {\"subject\": \"victory\", \"predicate\": \"be celebrated with\", \"object\": \"poem titled 'Oda a Platko'\"},\n",
    "  {\"subject\": \"'Oda a Platko'\", \"predicate\": \"be written by\", \"object\": \"Rafael Alberti\"},\n",
    "  {\"subject\": \"Rafael Alberti\", \"predicate\": \"be\", \"object\": \"member of Generation of '27\"},\n",
    "  {\"subject\": \"poem\", \"predicate\": \"be inspired by\", \"object\": \"heroic performance of Barcelona goalkeeper Franz Platko\"},\n",
    "  {\"subject\": \"Barcelona\", \"predicate\": \"win\", \"object\": \"inaugural Spanish League\"},\n",
    "  {\"subject\": \"win\", \"predicate\": \"occur\", \"object\": \"23 June 1929\"},\n",
    "  {\"subject\": \"Gamper\", \"predicate\": \"commit\", \"object\": \"suicide\"},\n",
    "  {\"subject\": \"suicide\", \"predicate\": \"occur\", \"object\": \"30 July 1930\"},\n",
    "  {\"subject\": \"depression\", \"predicate\": \"be caused by\", \"object\": \"personal and financial problems\"},\n",
    "  {\"subject\": \"depression\", \"predicate\": \"precede\", \"object\": \"Gamper's suicide\"}\n",
    "]\n",
    "\n",
    "filtered = filter_triplets_with_dates_or_numbers(triplets)\n",
    "for t in filtered:\n",
    "    print(t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:01:59.546033Z",
     "iopub.status.busy": "2025-05-21T13:01:59.545600Z",
     "iopub.status.idle": "2025-05-21T13:01:59.555965Z",
     "shell.execute_reply": "2025-05-21T13:01:59.554424Z",
     "shell.execute_reply.started": "2025-05-21T13:01:59.546001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between what year's did Victorian aboriginal groups dispossessed? -> Date\n",
      "In 1844 how many Aborigines resident in squalid camps in Melbourne? -> Date\n",
      "Who appointed five Aboriginal Protectors for the Aborigines of Victoria in 1839? -> Person\n",
      "Who were the people that were power political and economic force in Victoria in 1845? -> Person\n",
      "How many Aborigines were said to be resident in squalid camps in Melbourne in January 1844? -> Date\n",
      "How many Aboriginal Protectors for the Aborigines of Victoria were appointed in 1839? -> Date\n",
      "By what year did fewer than 240 wealthy Europeans hold all the pastoral licenses? -> Date\n",
      "During what years were Victorian Aboriginal groups largely displaced from their land? -> Date\n"
     ]
    }
   ],
   "source": [
    "def classify_question(question: str) -> str:\n",
    "    q = question.lower()\n",
    "\n",
    "    # Person-focused questions\n",
    "    if re.search(r\"\\bwho\\b\", q):\n",
    "        return \"Person\"\n",
    "    if re.search(r\"\\bwho (wrote|composed|created|painted|founded|led|directed)\\b\", q):\n",
    "        return \"Person\"\n",
    "\n",
    "    # Date-focused questions (more specific)\n",
    "    if re.search(r\"\\b(on what (day|date|year)|what year|when|how long|how many|how much|what years)\\b\", q):\n",
    "        return \"Date\"\n",
    "    if re.match(r\"^in \\d{4}\", q) and not re.search(r\"\\bwhat (incident|event|happened|caused)\\b\", q):\n",
    "        return \"Date\"\n",
    "\n",
    "    # Default fallback\n",
    "    return \"Other\"\n",
    "\n",
    "questions = [\"Between what year's did Victorian aboriginal groups dispossessed?\", 'In 1844 how many Aborigines resident in squalid camps in Melbourne?', 'Who appointed five Aboriginal Protectors for the Aborigines of Victoria in 1839?', 'Who were the people that were power political and economic force in Victoria in 1845?', 'How many Aborigines were said to be resident in squalid camps in Melbourne in January 1844?', 'How many Aboriginal Protectors for the Aborigines of Victoria were appointed in 1839?', 'By what year did fewer than 240 wealthy Europeans hold all the pastoral licenses?', 'During what years were Victorian Aboriginal groups largely displaced from their land?']\n",
    "for q in questions:\n",
    "    print(f\"{q} -> {classify_question(q)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the entities and the Dates here from the User question as the first step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:02:01.631548Z",
     "iopub.status.busy": "2025-05-21T13:02:01.630774Z",
     "iopub.status.idle": "2025-05-21T13:02:01.649788Z",
     "shell.execute_reply": "2025-05-21T13:02:01.648163Z",
     "shell.execute_reply.started": "2025-05-21T13:02:01.631508Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def extract_entities_dates_predicate_groq(question):\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Updated system prompt for clarity on 'entities' being a list of strings\n",
    "    system_prompt_content = \"\"\"You are an expert at extracting structured information. Your goal is to populate a JSON object with 'entities', 'dates', and a 'predicate' based on the user's question.\n",
    "\n",
    "Follow these rules precisely:\n",
    "1.  **Identify Subjects for \"entities\"**:\n",
    "    * Mentally break down the question to identify the main subject(s) of the core action or description. These subjects will be your \"entities\".\n",
    "    * Subjects can be proper nouns (e.g., \"Obama\", \"Paris\"), or general noun phrases (e.g., \"the man\", \"the red car\", \"companies\", \"the project manager\").\n",
    "    * If a question asks about \"who\" or \"what\" performed an action and the actor/subject is specified in the question, list that actor/subject.\n",
    "    * If the subject is implicit (e.g., in \"what happened?\"), or if the question asks \"who\" or \"what\" and the subject is the answer being sought, the \"entities\" list should typically be empty.\n",
    "2.  **Extract \"dates\"**:\n",
    "    * Extract ALL date mentions in any format (e.g., \"2nd March 2020\", \"May 2012\", \"2020\", \"yesterday\", \"last Monday\", \"first day\", \"Q2 2023\").\n",
    "    * Preserve original textual representation of dates, including ordinals (e.g., \"2nd\", \"24th\", \"1st\"). This should be a list of strings.\n",
    "3.  **Determine \"predicate\"**:\n",
    "    * Identify the main verb or action phrase that describes the central event or the relationship involving the primary subject(s). This will be the \"predicate\". This should be a single string.\n",
    "    * For questions about roles or states (e.g., \"who is the CEO of X\"), the predicate might be a phrase like \"is CEO of\" or simply \"is\".\n",
    "4.  **Output Format**:\n",
    "    * Return a single, valid JSON object.\n",
    "    * The JSON object must have exactly these three keys:\n",
    "        * \"entities\": A list of strings. This field **must always be a list**, even if it contains only one subject (e.g., `[\"subject1\"]`) or is empty (e.g., `[]`). Each string in the list should be a distinct, non-empty entity/subject.\n",
    "        * \"dates\": A list of strings. Each string should be an extracted date phrase.\n",
    "        * \"predicate\": A single string (or null if not clearly applicable).\n",
    "\"\"\"\n",
    "\n",
    "    user_prompt_content = f\"\"\"Extract from this question: entities, dates, and predicate.\n",
    "Examples:\n",
    "\"what happened on 2nd March 2020?\"  {{\"entities\": [], \"dates\": [\"2nd March 2020\"], \"predicate\": \"happened\"}}\n",
    "\"what did Obama say in May 2012?\"  {{\"entities\": [\"Obama\"], \"dates\": [\"May 2012\"], \"predicate\": \"say\"}}\n",
    "\"What tasks did the project manager assign last Monday?\"  {{\"entities\": [\"the project manager\"], \"dates\": [\"last Monday\"], \"predicate\": \"assign\"}}\n",
    "\"When was the old bridge closed for repairs?\"  {{\"entities\": [\"the old bridge\"], \"dates\": [\"When\"], \"predicate\": \"closed for repairs\"}} \n",
    "\n",
    "Question: \"{question}\"\n",
    "\"\"\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"llama-3.1-8b-instant\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt_content\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_prompt_content\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 400,\n",
    "        \"response_format\": {\"type\": \"json_object\"}\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, headers=headers, json=payload)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        content = data['choices'][0]['message']['content']\n",
    "        \n",
    "        extracted = None\n",
    "        try:\n",
    "            extracted = json.loads(content)\n",
    "        except json.JSONDecodeError:\n",
    "            match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "            if match:\n",
    "                extracted = json.loads(match.group())\n",
    "            else:\n",
    "                raise ValueError(\"No valid JSON found in response\")\n",
    "\n",
    "        # Robust handling for 'entities' to ensure it's a list of non-empty strings\n",
    "        final_entities = []\n",
    "        raw_entities_value = extracted.get(\"entities\")\n",
    "        if isinstance(raw_entities_value, str):\n",
    "            if raw_entities_value.strip(): # If it's a non-empty string\n",
    "                final_entities.append(raw_entities_value.strip())\n",
    "        elif isinstance(raw_entities_value, list):\n",
    "            for item in raw_entities_value:\n",
    "                if isinstance(item, str) and item.strip(): # Ensure item is a non-empty string\n",
    "                    final_entities.append(item.strip())\n",
    "        # If raw_entities_value is None, or an empty string/list, or contains only non-strings/empty-strings,\n",
    "        # final_entities will remain an empty list as initialized.\n",
    "\n",
    "        # Process dates\n",
    "        final_dates = []\n",
    "        if \"dates\" in extracted and extracted[\"dates\"]:\n",
    "            for date_text in extracted.get(\"dates\", []):\n",
    "                if date_text: \n",
    "                    norm = normalize_date(str(date_text)) \n",
    "                    if norm:\n",
    "                        final_dates.append(norm)\n",
    "        \n",
    "        if not final_dates:\n",
    "            final_dates.append({\"normalized_date\": None, \"type\": 4, \"original\": None})\n",
    "\n",
    "        return {\n",
    "            \"entities\": final_entities, # Use the robustly processed list of strings\n",
    "            \"dates\": final_dates,\n",
    "            \"predicate\": extracted.get(\"predicate\", None)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(f\"Error in Groq API call or processing: {str(e)}\") # Optional: uncomment for debugging\n",
    "        return {\n",
    "            \"entities\": [],\n",
    "            \"dates\": [{\"normalized_date\": None, \"type\": 4, \"original\": None}],\n",
    "            \"predicate\": None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Defining neo4j Basic search Methods Needed in Test Cases 3 and 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:02:04.538049Z",
     "iopub.status.busy": "2025-05-21T13:02:04.537683Z",
     "iopub.status.idle": "2025-05-21T13:02:04.556379Z",
     "shell.execute_reply": "2025-05-21T13:02:04.555104Z",
     "shell.execute_reply.started": "2025-05-21T13:02:04.538027Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    return np.dot(a, b) / (norm(a) * norm(b) + 1e-8)  # Prevent division by zero\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Strip accents and convert to ASCII\n",
    "    nfkd_form = unicodedata.normalize('NFKD', text)\n",
    "    return \"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "\n",
    "def extract_entities_llm(query):\n",
    "    prompt = (\n",
    "        \"Extract only the most relevant entity or keyword from the following question.\\n\"\n",
    "        \"Return just the name(s), without any extra text or explanation.\\n\\n\"\n",
    "        f\"Question: {query}\\n\\nEntities:\"\n",
    "    )\n",
    "    raw_entities = query_groq_llm(prompt).strip()\n",
    "\n",
    "    # Clean output: remove (tags), bullets, quotes, and extra whitespace\n",
    "    cleaned = re.sub(r\"\\s*\\([^)]*\\)\", \"\", raw_entities)     # Remove (type)\n",
    "    cleaned = re.sub(r\"^\\*+\", \"\", cleaned)                  # Remove bullets like \"* \"\n",
    "    cleaned = re.sub(r\"[\\\"'`]\", \"\", cleaned)                # Remove quotes\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()          # Normalize spaces\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def query_groq_llm(prompt):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.post(\"https://api.groq.com/openai/v1/chat/completions\", json=payload, headers=headers)\n",
    "    response_json =  response.json()\n",
    "    \n",
    "    if \"choices\" in response_json and response_json[\"choices\"] and \"message\" in response_json[\"choices\"][0] and \"content\" in response_json[\"choices\"][0][\"message\"]:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else : \n",
    "        return response_json\n",
    "\n",
    "\n",
    "def answer_query(query):\n",
    "    entities = extract_entities_llm(query)\n",
    "    entities = [normalize_text(e.strip()) for e in entities.split(\",\") if e.strip()]\n",
    "    #print(f\"[DEBUG] Extracted Entities: {entities}\")\n",
    "    \n",
    "    # 2. Generate embeddings for each entity\n",
    "    entity_embeddings = [embedder.encode(e).tolist() for e in entities]\n",
    "    \n",
    "    # 3. Fetch all candidate triplets with their embeddings\n",
    "    with driver.session() as session:\n",
    "        candidates = session.run(\n",
    "            \"\"\"\n",
    "            MATCH (s)-[r]->(o)\n",
    "            WHERE s.embedding IS NOT NULL AND o.embedding IS NOT NULL\n",
    "            RETURN s.name AS subject, \n",
    "                   r.type AS predicate, \n",
    "                   o.name AS object,\n",
    "                   s.embedding AS subj_emb,\n",
    "                   o.embedding AS obj_emb ,\n",
    "                   r.date  AS  date\n",
    "            \"\"\"\n",
    "        ).data()\n",
    "    #print(f\"candidates length --> {len(candidates)}\")\n",
    "    # 4. Calculate similarities in Python\n",
    "    all_triplets = set()\n",
    "    for entity, emb in zip(entities, entity_embeddings):\n",
    "        for record in candidates:\n",
    "            try:\n",
    "                subj_sim = cosine_similarity(emb, record[\"subj_emb\"])\n",
    "                obj_sim = cosine_similarity(emb, record[\"obj_emb\"])\n",
    "                \n",
    "                if subj_sim > 0.6 or obj_sim > 0.6:  # Similarity threshold\n",
    "                    triplet = f\"({record['subject']}) -[{record['predicate']}]-> ({record['object']} Date:{record['date']})\"\n",
    "                    all_triplets.add(triplet)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # 5. Prepare context\n",
    "    if not all_triplets:\n",
    "        triplet_str = \"No relevant triplets found.\"\n",
    "    else:\n",
    "        triplet_str = \"\\n\".join(all_triplets)\n",
    "    \n",
    "    #print(f\"[DEBUG] Context Triplets:\\n{triplet_str}\")\n",
    "    \n",
    "    final_prompt = f\"\"\"Answer this query: {query}\n",
    "    Using ONLY the following verified information from our knowledge graph:\n",
    "    {triplet_str}\n",
    "    \n",
    "    If no relevant information exists, respond \"I don't have verified information about this.\"\n",
    "    \"\"\"\n",
    "    \n",
    "    return query_groq_llm(final_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just Testing Out the connection to the neo4j to check whether there is a problem or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:02:06.906737Z",
     "iopub.status.busy": "2025-05-21T13:02:06.906386Z",
     "iopub.status.idle": "2025-05-21T13:02:17.219103Z",
     "shell.execute_reply": "2025-05-21T13:02:17.218006Z",
     "shell.execute_reply.started": "2025-05-21T13:02:06.906716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d5d5a886544d009c2a255598f7442b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a9115d6e06845ab833bef3ae4f5cfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'A challenging query!\\n\\nAfter carefully searching through the provided knowledge graph, I was able to extract some information related to the query \"what happened between Palestinians and Israel\". Here are the relevant points:\\n\\n1. **Israel and Palestinian territories share land borders**: (The country) -[shares]-> (land borders with the Palestinian territories Date:None)\\n\\n2. **Palestinian territories are partially controlled by Israel**: (Palestinian territories) -[are partially controlled by]-> (Israel Date:None)\\n\\n3. **Palestinian territories are claimed by the State of Palestine**: (Palestinian territories) -[are claimed by]-> (State of Palestine Date:None)\\n\\n4. **Jerusalem is a recognized capital by Israel, but internationally unrecognized**: (Jerusalem) -[is]-> (self-designated capital of Israel Date:None), (Israeli sovereignty over Jerusalem) -[is]-> (internationally unrecognized Date:None)\\n\\n5. **Israel is a sovereign state in Western Asia**: (Israel) -[is]-> (a sovereign state in Western Asia Date:None)\\n\\nUnfortunately, I couldn\\'t find any specific information about a particular event or conflict between Palestinians and Israel in the provided knowledge graph. If you\\'re looking for a specific event or topic, feel free to ask, and I\\'ll let you know if I have the information.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"what happend between Palestines and Israel ?\"\n",
    "answer_query(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Main Method that is going to handle all the 4 cases "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Case1 What happened on 1-1-2025 --> Use Manual Graph Search Strategy\n",
    "2.  Cas2 What happend on Jan 2025 --> Use Manual Graph Search Strategy but we have to loop over all the days in this month\n",
    "3.  Case3 What happened on 2025 --> Use neo4j semantic search\n",
    "4.  Case4 what happend   --> Use neo4j semantic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:02:17.220974Z",
     "iopub.status.busy": "2025-05-21T13:02:17.220643Z",
     "iopub.status.idle": "2025-05-21T13:02:17.238062Z",
     "shell.execute_reply": "2025-05-21T13:02:17.236425Z",
     "shell.execute_reply.started": "2025-05-21T13:02:17.220950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def answer_user_question(question,kg):\n",
    "    # --- Step 1: Extract structured info\n",
    "    question_type = classify_question(question)\n",
    "    print(question_type)\n",
    "    extracted = extract_entities_dates_predicate_groq(question)\n",
    "    if not extracted[\"entities\"] or not extracted[\"dates\"]:\n",
    "        return \"Sorry, I couldn't understand the question.\"\n",
    "\n",
    "    subject = extracted[\"entities\"][0]\n",
    "    \n",
    "    predicate = extracted[\"predicate\"]\n",
    "    date_info = extracted[\"dates\"][0]\n",
    "    print(\"subject \",subject)\n",
    "    print(\"verb \",predicate)\n",
    "    print(\"date \",date_info)\n",
    "    # --- Step 2: Determine which case we are going to answer (e.g. 1-2-3-4)\n",
    "    if date_info['type'] == 0 :\n",
    "        print(date_info[\"normalized_date\"])\n",
    "        triplet = kg.search(date_info[\"normalized_date\"], subject, predicate,question_type)\n",
    "        print(\"triplet: \",triplet)\n",
    "        if triplet == []:\n",
    "            return \"Sorry, I couldn't find the answer in the news data.\"\n",
    "        print(triplet[:5])\n",
    "        triplet = triplet[0]\n",
    "    elif date_info['type'] == 1 :\n",
    "        date = date_info['normalized_date'] \n",
    "        month , _ ,year = date.split('/') \n",
    "        month , year = int(month) , int(year)\n",
    "        triplet = kg.search_month(month, year,subject,predicate,question_type)[0]\n",
    "        print(triplet)\n",
    "        #print(\"triplet: \",triplet)\n",
    "        if triplet is None:\n",
    "            return \"Sorry, I couldn't find the answer in the news data.\"\n",
    "    elif date_info['type'] == 3 : \n",
    "        return answer_query(question)\n",
    "    else: \n",
    "        #print(question)\n",
    "        return answer_query(question)\n",
    "        \n",
    "    # --- Step 3: Use LLM to generate the final user answer\n",
    "    answer = generate_answer_from_triplet(question, triplet)\n",
    "    return answer\n",
    "\n",
    "def generate_answer_from_triplet(user_question, triplet):\n",
    "\n",
    "    subject, predicate, obj = triplet['subject'] , triplet['predicate'] , triplet['object']\n",
    "\n",
    "    url = \"https://api.groq.com/openai/v1/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {GROQ_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"llama3-70b-8192\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an assistant that ONLY generates short direct answers based on the given fact.\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"\n",
    "Using the information:\n",
    "\n",
    "Subject: {subject}\n",
    "Predicate: {predicate}\n",
    "Object: {obj}\n",
    "\n",
    "Write a very short natural sentence. Do NOT add explanations, do NOT say you are sorry. Only say the main information.\n",
    "\n",
    "Example format:\n",
    "\"[subject] [predicate] [object]\"\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.2,\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "\n",
    "    final_answer = data['choices'][0]['message']['content']\n",
    "    return final_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We are Going to test The 4 cases on 3 questions and here are their GT Answers Respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Poultry is the second most widely eaten type of meat globally\n",
    "\n",
    "\n",
    "\n",
    "2. allowing people to self-identify as more than one ethnicity\n",
    "\n",
    "\n",
    "3. subjects had more difficulty recalling collections of letters that were acoustically similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1  MM/DD/YYYY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:02:17.243157Z",
     "iopub.status.busy": "2025-05-21T13:02:17.240241Z",
     "iopub.status.idle": "2025-05-21T13:02:17.970409Z",
     "shell.execute_reply": "2025-05-21T13:02:17.968440Z",
     "shell.execute_reply.started": "2025-05-21T13:02:17.243103Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  poultry\n",
      "verb  is\n",
      "date  {'normalized_date': '03/16/2018', 'type': 0, 'original': '16 March 2018'}\n",
      "03/16/2018\n",
      "triplet:  [{'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': ['is'], 'hops': 1, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'Poultry', 'path': ['is', 'is'], 'hops': 2, 'total_score': np.float32(1.9999999)}]\n",
      "[{'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': ['is'], 'hops': 1, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'Poultry', 'path': ['is', 'is'], 'hops': 2, 'total_score': np.float32(1.9999999)}]\n",
      "Predicted anwer using temporal graph rag :  Poultry is the second most widely eaten type of meat globally.\n",
      "ground truth answer : Poultry is the second most widely eaten type of meat globally\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question(\"How popular is poultry as a consumable among humans in 16 March 2018 ?\",Manual_Graph))\n",
    "print(\"ground truth answer : Poultry is the second most widely eaten type of meat globally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  the Census Bureau\n",
      "verb  changed its collection of data\n",
      "date  {'normalized_date': '03/03/2018', 'type': 0, 'original': '3 March 2018'}\n",
      "03/03/2018\n",
      "triplet:  [{'subject': 'Census Bureau', 'subject_score': np.float32(0.97950697), 'predicate': 'changed', 'predicate_score': 0.4662553071975708, 'object': 'its data collection', 'path': None, 'hops': 0, 'total_score': np.float32(1.4457623)}]\n",
      "[{'subject': 'Census Bureau', 'subject_score': np.float32(0.97950697), 'predicate': 'changed', 'predicate_score': 0.4662553071975708, 'object': 'its data collection', 'path': None, 'hops': 0, 'total_score': np.float32(1.4457623)}]\n",
      "Predicted anwer using temporal graph rag :  The Census Bureau changed its data collection.\n",
      "ground truth answer : The Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question(\"How had the Census Bureau changed its collection of data in 3 March 2018 ?\",Manual_Graph))\n",
    "print(\"ground truth answer : The Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  conrad\n",
      "verb  find\n",
      "date  {'normalized_date': '03/02/2018', 'type': 0, 'original': '2 March 2018'}\n",
      "03/02/2018\n",
      "triplet:  [{'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'test subjects had more difficulty recalling collections of letters that were acoustically similar', 'path': None, 'hops': 0, 'total_score': np.float32(1.4679174)}, {'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'test subjects had more difficulty recalling collections of letters that were acoustically similar', 'path': ['found'], 'hops': 1, 'total_score': np.float32(1.4679174)}, {'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'Conrad (1964)', 'path': ['found', 'found'], 'hops': 2, 'total_score': np.float32(1.4679174)}]\n",
      "[{'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'test subjects had more difficulty recalling collections of letters that were acoustically similar', 'path': None, 'hops': 0, 'total_score': np.float32(1.4679174)}, {'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'test subjects had more difficulty recalling collections of letters that were acoustically similar', 'path': ['found'], 'hops': 1, 'total_score': np.float32(1.4679174)}, {'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'Conrad (1964)', 'path': ['found', 'found'], 'hops': 2, 'total_score': np.float32(1.4679174)}]\n",
      "Predicted anwer using temporal graph rag :  Conrad found test subjects had more difficulty recalling collections of letters that were acoustically similar.\n",
      "ground truth answer : subjects had more difficulty recalling collections of letters that were acoustically similar\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question('What did conrad find about test subjects in 2 March 2018?',Manual_Graph))\n",
    "print(\"ground truth answer : subjects had more difficulty recalling collections of letters that were acoustically similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2 month/year  we need to loop over all days and call search function (some months are 28 days others are 31 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  poultry\n",
      "verb  is\n",
      "date  {'normalized_date': '03/16/2018', 'type': 0, 'original': '16 March 2018'}\n",
      "03/16/2018\n",
      "triplet:  [{'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': ['is'], 'hops': 1, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'Poultry', 'path': ['is', 'is'], 'hops': 2, 'total_score': np.float32(1.9999999)}]\n",
      "[{'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': None, 'hops': 0, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'second most widely eaten type of meat globally', 'path': ['is'], 'hops': 1, 'total_score': np.float32(1.9999999)}, {'subject': 'Poultry', 'subject_score': np.float32(0.9999999), 'predicate': 'is', 'predicate_score': 1.0, 'object': 'Poultry', 'path': ['is', 'is'], 'hops': 2, 'total_score': np.float32(1.9999999)}]\n",
      "Predicted anwer using temporal graph rag :  Poultry is the second most widely eaten type of meat globally.\n",
      "ground truth answer : Poultry is the second most widely eaten type of meat globally\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question(\"How popular is poultry as a consumable among humans in March 2018 ?\",Manual_Graph))\n",
    "print(\"ground truth answer : Poultry is the second most widely eaten type of meat globally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-21T13:11:18.614313Z",
     "iopub.status.busy": "2025-05-21T13:11:18.612943Z",
     "iopub.status.idle": "2025-05-21T13:11:19.220145Z",
     "shell.execute_reply": "2025-05-21T13:11:19.218297Z",
     "shell.execute_reply.started": "2025-05-21T13:11:18.614269Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  the Census Bureau\n",
      "verb  changed its collection of data\n",
      "date  {'normalized_date': '03/03/2018', 'type': 0, 'original': '3 March 2018'}\n",
      "03/03/2018\n",
      "triplet:  [{'subject': 'Census Bureau', 'subject_score': np.float32(0.97950697), 'predicate': 'changed', 'predicate_score': 0.4662553071975708, 'object': 'its data collection', 'path': None, 'hops': 0, 'total_score': np.float32(1.4457623)}]\n",
      "[{'subject': 'Census Bureau', 'subject_score': np.float32(0.97950697), 'predicate': 'changed', 'predicate_score': 0.4662553071975708, 'object': 'its data collection', 'path': None, 'hops': 0, 'total_score': np.float32(1.4457623)}]\n",
      "Predicted anwer using temporal graph rag :  The Census Bureau changed its data collection.\n",
      "ground truth answer : The Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question(\"How had the Census Bureau changed its collection of data in March 2018 ?\",Manual_Graph))\n",
    "print(\"ground truth answer : The Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  conrad\n",
      "verb  find\n",
      "date  {'normalized_date': '03/01/2018', 'type': 1, 'original': 'March 2018'}\n",
      "{'subject': 'Conrad (1964)', 'subject_score': np.float32(0.82585865), 'predicate': 'found', 'predicate_score': 0.642058789730072, 'object': 'test subjects had more difficulty recalling collections of letters that were acoustically similar', 'path': None, 'hops': 0, 'total_score': np.float32(1.4679174)}\n",
      "Predicted anwer using temporal graph rag :  Conrad found test subjects had more difficulty recalling collections of letters that were acoustically similar.\n",
      "ground truth answer : subjects had more difficulty recalling collections of letters that were acoustically similar\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted anwer using temporal graph rag : \",answer_user_question('What did conrad find about test subjects in March 2018?',Manual_Graph))\n",
    "print(\"ground truth answer : subjects had more difficulty recalling collections of letters that were acoustically similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3 just year  --> do semantic search using neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:05:12.605608Z",
     "iopub.status.busy": "2025-05-21T13:05:12.605187Z",
     "iopub.status.idle": "2025-05-21T13:05:22.005599Z",
     "shell.execute_reply": "2025-05-21T13:05:22.004373Z",
     "shell.execute_reply.started": "2025-05-21T13:05:12.605582Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  poultry\n",
      "verb  is popular\n",
      "date  {'normalized_date': '01/01/2018', 'type': 3, 'original': '2018'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d00f267228a49d8bab371d06b7abb65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to the provided verified information, poultry is the second most widely eaten type of meat globally, implying a high level of consumption among humans. No specific data or statistics for 2018 are available in the provided knowledge graph.'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"How popular is poultry as a consumable among humans in 2018?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:05:56.416628Z",
     "iopub.status.busy": "2025-05-21T13:05:56.416185Z",
     "iopub.status.idle": "2025-05-21T13:06:05.676128Z",
     "shell.execute_reply": "2025-05-21T13:06:05.675055Z",
     "shell.execute_reply.started": "2025-05-21T13:05:56.416601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  the Census Bureau\n",
      "verb  changed its collection of data\n",
      "date  {'normalized_date': '01/01/2018', 'type': 3, 'original': '2018'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafd94618ee34a298b6cca03740f0ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to the provided knowledge graph, the answer to the query is:\\n\\nThe Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity.\\n\\nThis information is verified and extracted from the knowledge graph.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"How had the Census Bureau changed its collection of data in 2018?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:07:15.230582Z",
     "iopub.status.busy": "2025-05-21T13:07:15.229538Z",
     "iopub.status.idle": "2025-05-21T13:07:24.654182Z",
     "shell.execute_reply": "2025-05-21T13:07:24.652854Z",
     "shell.execute_reply.started": "2025-05-21T13:07:15.230530Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  conrad\n",
      "verb  find\n",
      "date  {'normalized_date': '01/01/2018', 'type': 3, 'original': '2018'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb9fb03461a461ebaee0231811e4f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"I don't have verified information about this.\\n\\nThe provided knowledge graph only contains information about a 1964 study, where Conrad found that test subjects had more difficulty recalling collections of letters that were acoustically similar. There is no information about Conrad's findings in 2018.\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"What did conrad find about test subjects in 2018?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4 no date at all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:08:46.137049Z",
     "iopub.status.busy": "2025-05-21T13:08:46.136636Z",
     "iopub.status.idle": "2025-05-21T13:08:54.961336Z",
     "shell.execute_reply": "2025-05-21T13:08:54.960164Z",
     "shell.execute_reply.started": "2025-05-21T13:08:46.137021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  poultry\n",
      "verb  is\n",
      "date  {'normalized_date': None, 'type': 4, 'original': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf36e185b134dd5ac3f81b38bfc2f4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to our verified knowledge graph, poultry is the \"second most widely eaten type of meat globally\". This suggests that poultry is a very popular consumable among humans.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"How popular is poultry as a consumable among humans?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:09:12.519221Z",
     "iopub.status.busy": "2025-05-21T13:09:12.518725Z",
     "iopub.status.idle": "2025-05-21T13:09:21.771934Z",
     "shell.execute_reply": "2025-05-21T13:09:21.770147Z",
     "shell.execute_reply.started": "2025-05-21T13:09:12.519184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  the Census Bureau\n",
      "verb  changed its collection of data\n",
      "date  {'normalized_date': None, 'type': 4, 'original': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce25e5f246ea4f9e82b746b33397dd88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Based on the provided knowledge graph, the Census Bureau has changed its collection of data in the following ways:\\n\\n* The Census Bureau changed its data collection by allowing people to self-identify as more than one ethnicity.\\n* The Census 2000 asked about race differently than previous censuses.\\n* The Census Bureau included more than a dozen ethnic/racial categories on the census by 1990.\\n* The Census Bureau changed its data collection, and as a result, the data from the Census 2000 is not directly comparable with data from the 1990 census or earlier censuses.\\n\\nThese changes reflect changing social ideas about ethnicity and a wide variety of immigrants who came to the United States.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"How had the Census Bureau changed its collection of data?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-21T13:09:32.437716Z",
     "iopub.status.busy": "2025-05-21T13:09:32.437183Z",
     "iopub.status.idle": "2025-05-21T13:09:40.916581Z",
     "shell.execute_reply": "2025-05-21T13:09:40.915453Z",
     "shell.execute_reply.started": "2025-05-21T13:09:32.437682Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Other\n",
      "subject  conrad\n",
      "verb  find about\n",
      "date  {'normalized_date': None, 'type': 4, 'original': None}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2500381e38cf40d783749861ab42bba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'According to the provided knowledge graph, Conrad found that test subjects had more difficulty recalling collections of letters that were acoustically similar.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_user_question(\"What did conrad find about test subjects?\",Manual_Graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_rag(dataset_path, answer_query_func):\n",
    "    \"\"\"\n",
    "    Evaluates a RAG model using a given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_path (str): Path to the CSV dataset file.\n",
    "        answer_query_func (function): The function that takes a query string\n",
    "                                       and returns a generated answer string.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the computed evaluation scores.\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {dataset_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(dataset_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset not found at {dataset_path}\")\n",
    "        return None\n",
    "\n",
    "    # Determine column names dynamically\n",
    "    question_col = None\n",
    "    answer_col = None\n",
    "\n",
    "    if 'question' in df.columns and 'answer' in df.columns:\n",
    "        question_col = 'question'\n",
    "        answer_col = 'answer'\n",
    "    elif 'Question' in df.columns and 'Answer' in df.columns:\n",
    "        question_col = 'Question'\n",
    "        answer_col = 'Answer'\n",
    "    else:\n",
    "        print(f\"Error: Could not find 'question'/'answer' or 'Question'/'Answer' columns in {dataset_path}\")\n",
    "        print(f\"Available columns: {df.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    generated_answers = []\n",
    "    ground_truth_answers = []\n",
    "    questions = []\n",
    "    x = 0\n",
    "    print(f\"Processing {len(df)} questions...\")\n",
    "    # Iterate through the dataset and generate answers\n",
    "    for index, row in df.iterrows():\n",
    "        question = row[question_col]\n",
    "        ground_truth = str(row[answer_col]) # Ensure ground truth is string\n",
    "        questions.append(question)\n",
    "        ground_truth_answers.append(ground_truth)\n",
    "\n",
    "        # Generate answer using the provided function\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            generated_answer = str(answer_query_func(question , sohailo)) # Ensure generated answer is string\n",
    "            generated_answers.append(generated_answer)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer for question: {question[:50]}... - {e}\")\n",
    "            generated_answers.append(\"\") # Append empty string on error\n",
    "        x = x+1\n",
    "        if(x > 40):\n",
    "            break\n",
    "\n",
    "\n",
    "    # --- Compute Evaluation Metrics ---\n",
    "\n",
    "    # BLEU-4 Score\n",
    "    # BLEU requires tokenized sentences\n",
    "    # Smoothing function is used to handle cases where n-grams are not found\n",
    "    df = pd.DataFrame({\n",
    "    \"Question\": questions,\n",
    "    \"Generated Answer\": generated_answers,\n",
    "    \"Ground Truth\": ground_truth_answers\n",
    "    })\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(\"qa_results.csv\", index=False)\n",
    "    \n",
    "    print(\"Saved to qa_results.csv\")\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu4_scores = []\n",
    "    for ref, gen in zip(ground_truth_answers, generated_answers):\n",
    "        # BLEU expects a list of reference sentences (even if only one)\n",
    "        reference_tokens = [nltk.word_tokenize(ref)]\n",
    "        candidate_tokens = nltk.word_tokenize(gen)\n",
    "        score = sentence_bleu(reference_tokens, candidate_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "        bleu4_scores.append(score)\n",
    "\n",
    "    average_bleu4 = sum(bleu4_scores) / len(bleu4_scores) if bleu4_scores else 0\n",
    "\n",
    "\n",
    "    # ROUGE-L Score\n",
    "    # ROUGE expects strings\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rougeL_scores = []\n",
    "    for ref, gen in zip(ground_truth_answers, generated_answers):\n",
    "        scores = scorer.score(ref, gen)\n",
    "        rougeL_scores.append(scores['rougeL'].fmeasure) # Using f-measure for ROUGE-L\n",
    "\n",
    "    average_rougeL = sum(rougeL_scores) / len(rougeL_scores) if rougeL_scores else 0\n",
    "\n",
    "    # You can add other metrics here if needed\n",
    "\n",
    "    evaluation_results = {\n",
    "        \"Average BLEU-4\": average_bleu4,\n",
    "        \"Average ROUGE-L (F-measure)\": average_rougeL,\n",
    "    }\n",
    "\n",
    "    return evaluation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path_1 = '/kaggle/input/sohail-test-data/all_qa.csv'\n",
    "print(\"--- Evaluating Dataset 1 ---\")\n",
    "results_dataset1 = evaluate_rag(dataset_path_1, answer_user_question)\n",
    "if results_dataset1:\n",
    "    print(\"\\n--- Results for /kaggle/input/sohail-test-data/all_qa.csv ---\")\n",
    "    for metric, score in results_dataset1.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"\\nEvaluation failed for /kaggle/input/sohail-test-data/all_qa.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_path_2 = '/kaggle/input/yara-qa/test_data_2.csv'\n",
    "print(\"\\n--- Evaluating Dataset 2 ---\")\n",
    "results_dataset2 = evaluate_rag(dataset_path_2, answer_user_question)\n",
    "\n",
    "if results_dataset2:\n",
    "    print(\"\\n--- Results for /kaggle/input/yara-qa/test_data_2.csv ---\")\n",
    "    for metric, score in results_dataset2.items():\n",
    "        print(f\"{metric}: {score:.4f}\")\n",
    "else:\n",
    "     print(\"\\nEvaluation failed for /kaggle/input/yara-qa/test_data_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7135402,
     "sourceId": 11393392,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7221248,
     "sourceId": 11515144,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7277529,
     "sourceId": 11603369,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7336215,
     "sourceId": 11688442,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7406544,
     "sourceId": 11794936,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7429717,
     "sourceId": 11827037,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7429851,
     "sourceId": 11827226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7430006,
     "sourceId": 11827451,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7473192,
     "sourceId": 11889826,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 233542880,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 239552842,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 343961,
     "modelInstanceId": 323214,
     "sourceId": 392558,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 349519,
     "modelInstanceId": 328684,
     "sourceId": 401754,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
