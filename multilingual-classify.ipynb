{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9463639,"sourceType":"datasetVersion","datasetId":5754014},{"sourceId":9463778,"sourceType":"datasetVersion","datasetId":5754115},{"sourceId":9464571,"sourceType":"datasetVersion","datasetId":5754691},{"sourceId":9464638,"sourceType":"datasetVersion","datasetId":5754745},{"sourceId":197956292,"sourceType":"kernelVersion"}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"Jinyan1/COLING_2025_MGT_multingual\")","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:28:25.575736Z","iopub.execute_input":"2024-09-23T21:28:25.576407Z","iopub.status.idle":"2024-09-23T21:28:45.418546Z","shell.execute_reply.started":"2024-09-23T21:28:25.576367Z","shell.execute_reply":"2024-09-23T21:28:45.417742Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/589 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e7e446aa3f461b9ef45a8f59631b8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cda24c36390e4fca8eeb89a029938969"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cae3ba1454b49d9bd8d3bd18dc75494"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ef334ea20e441198c688e4702d18fc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/275M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d12def2fb24042f7a606c0cb5dbe456f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/674083 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"426d96ebd2234389a9e82aad459b33be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating dev split:   0%|          | 0/288894 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b05138f23bd4b92812eb062873cdd52"}},"metadata":{}}]},{"cell_type":"code","source":"print(ds)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:28:45.420119Z","iopub.execute_input":"2024-09-23T21:28:45.420601Z","iopub.status.idle":"2024-09-23T21:28:45.425801Z","shell.execute_reply.started":"2024-09-23T21:28:45.420567Z","shell.execute_reply":"2024-09-23T21:28:45.424683Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n        num_rows: 674083\n    })\n    dev: Dataset({\n        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n        num_rows: 288894\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas\n\ntrain_df = ds['train'].to_pandas()\ndev_df = ds['dev'].to_pandas()\n\n# Verify the conversion\nprint(train_df.head())\nprint(dev_df.head())","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:28:45.426953Z","iopub.execute_input":"2024-09-23T21:28:45.427710Z","iopub.status.idle":"2024-09-23T21:28:49.172534Z","shell.execute_reply.started":"2024-09-23T21:28:45.427669Z","shell.execute_reply":"2024-09-23T21:28:49.171521Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"                                     id source   sub_source lang        model  \\\n0  808a846d-41c1-46ff-80ab-892b01bec5f3   m4gt        arxiv   en  gemma-7b-it   \n1  673ec6fc-624a-4ad5-8d5f-a5950b0e5432   mage           wp   en        human   \n2  3059e6e4-1724-4745-a833-cbb6bfb4acd8   mage         xsum   en   flan_t5_xl   \n3  4ca1cde7-6dbd-4519-b60c-7f8c8fcd0d36    hc3      open_qa   zh       gpt-35   \n4  f9cca231-ed31-45a9-a448-99245f72e32c    hc3  reddit_eli5   en       gpt-35   \n\n   label                                               text  \n0      1  This report summarizes the findings of the US ...  \n1      0  I've been standing here for days now. Watching...  \n2      1  Towell, 25, was knocked down twice during the ...  \n3      1  我不确定你想要问的是什么，但是阿玛尼是一个奢侈品牌，它的产品包括化妆品、香水和时装。黑手党是...  \n4      1  Sometimes when we eat certain types of food, o...  \n                                     id source        sub_source lang   model  \\\n0  8a6ce8d6-a07c-4a2b-831e-db606a7d46ad   mage                wp   en      7B   \n1  e811404e-fd66-4e58-a2d4-e0e3d5a4b38b   mage              eli5   en   human   \n2  963c8b72-a7d1-4af8-babd-20dc67248e6d    hc3       reddit_eli5   en   human   \n3  c9bf362f-6329-405d-9b86-3a6312179eb3   m4gt  True & Fake News   bg   human   \n4  a61f9bd7-58ec-4339-ac15-c107d2a0f9e8   mage             squad   en  t0_11b   \n\n   label                                               text  \n0      1  The United Nations declared war has been illeg...  \n1      0  Horse bones are incredibly dense and under a m...  \n2      0  Balls originally form in the abdomen . And eve...  \n3      0  Несъгласие с предложенията на служебния минист...  \n4      1  Temujin began his ascent to power by offering ...  \n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport numpy as np\n\n# Ensure TensorFlow uses the GPU\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\n# Load the tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\nbert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n# Assuming you already have 'train_df' and 'dev_df' converted from DatasetDict\n# Extract text, lang, and label columns from the DataFrame\ntrain_texts = train_df['text'].tolist()\ntrain_langs = train_df['lang'].tolist()\ntrain_labels = train_df['label'].values\n\ndev_texts = dev_df['text'].tolist()\ndev_langs = dev_df['lang'].tolist()\ndev_labels = dev_df['label'].values\n\n# Tokenize the text input with tqdm progress tracking\nmax_length = 128  # Maximum length for the tokenized sequences","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:28:49.176457Z","iopub.execute_input":"2024-09-23T21:28:49.176759Z","iopub.status.idle":"2024-09-23T21:29:17.719130Z","shell.execute_reply.started":"2024-09-23T21:28:49.176726Z","shell.execute_reply":"2024-09-23T21:29:17.718268Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Num GPUs Available:  1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7b366a072d4b80844019c2187b9eec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b35892425d4e24b3f398f58f20ca61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c01b4325c864f1180f2cb9438bccc70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6906c4670d64df6b230ad29c32b66fe"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5615dda9a70e474ba90c59b4d99dc1cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Tokenizing train data with progress bar\ntrain_encodings = {\n    'input_ids': [],\n    'attention_mask': []\n}\n\nfor text in tqdm(train_texts, desc=\"Tokenizing train data\"):\n    enc = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n    train_encodings['input_ids'].append(enc['input_ids'])\n    train_encodings['attention_mask'].append(enc['attention_mask'])\n\n# Convert lists to tensors\ntrain_encodings['input_ids'] = tf.concat(train_encodings['input_ids'], axis=0)\ntrain_encodings['attention_mask'] = tf.concat(train_encodings['attention_mask'], axis=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_encodings['attention_mask']))","metadata":{"execution":{"iopub.status.busy":"2024-09-23T12:48:03.684989Z","iopub.execute_input":"2024-09-23T12:48:03.685453Z","iopub.status.idle":"2024-09-23T12:48:03.692932Z","shell.execute_reply.started":"2024-09-23T12:48:03.685413Z","shell.execute_reply":"2024-09-23T12:48:03.691438Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"674083\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import BertTokenizer, TFBertModel\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm import tqdm\nimport numpy as np\n\nmax_length = 128  # Maximum length for the tokenized sequences\n\n\n# Tokenizing dev data with progress bar\ndev_encodings = {\n    'input_ids': [],\n    'attention_mask': []\n}\n\nfor text in tqdm(dev_texts, desc=\"Tokenizing dev data\"):\n    enc = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n    dev_encodings['input_ids'].append(enc['input_ids'])\n    dev_encodings['attention_mask'].append(enc['attention_mask'])\n\n# Convert lists to tensors\ndev_encodings['input_ids'] = tf.concat(dev_encodings['input_ids'], axis=0)\ndev_encodings['attention_mask'] = tf.concat(dev_encodings['attention_mask'], axis=0)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T15:05:28.505047Z","iopub.execute_input":"2024-09-23T15:05:28.508192Z","iopub.status.idle":"2024-09-23T15:52:47.449615Z","shell.execute_reply.started":"2024-09-23T15:05:28.508080Z","shell.execute_reply":"2024-09-23T15:52:47.448024Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Tokenizing dev data: 100%|██████████| 288894/288894 [46:33<00:00, 103.43it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle\n\nwith open('/kaggle/input/train-ids/input_ids.pkl', 'rb') as file:\n    train_input_ids = pickle.load(file)\nwith open('/kaggle/input/train-attention/attention_masks.pkl', 'rb') as file:\n    train_encodings_attention = pickle.load(file)\nwith open('/kaggle/input/dev-attention/dev_attention.pkl', 'rb') as file:\n    dev_encodings_attention = pickle.load(file)\nwith open('/kaggle/input/dev-input-ids/dev_input_ids.pkl', 'rb') as file:\n    dev_input_ids = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:29:17.720881Z","iopub.execute_input":"2024-09-23T21:29:17.721580Z","iopub.status.idle":"2024-09-23T21:29:25.691329Z","shell.execute_reply.started":"2024-09-23T21:29:17.721543Z","shell.execute_reply":"2024-09-23T21:29:25.689905Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# One-hot encode the language input (using sklearn OneHotEncoder)\nencoder = OneHotEncoder(sparse = False)\ntrain_langs_encoded = encoder.fit_transform(np.array(train_langs).reshape(-1, 1))\ndev_langs_encoded = encoder.transform(np.array(dev_langs).reshape(-1, 1))\n\nnum_languages = train_langs_encoded.shape[1]","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:29:25.693272Z","iopub.execute_input":"2024-09-23T21:29:25.693665Z","iopub.status.idle":"2024-09-23T21:29:27.227324Z","shell.execute_reply.started":"2024-09-23T21:29:25.693619Z","shell.execute_reply":"2024-09-23T21:29:27.226482Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFBertModel\n\n# Define BERT model with attention masks\nbert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n\n# Input layers\ninput_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\nattention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n\n# Get BERT embeddings using input_ids and attention_mask\nbert_outputs = bert_model(input_ids, attention_mask=attention_mask)\ntext_embeddings = tf.keras.layers.GlobalAveragePooling1D()(bert_outputs.last_hidden_state)\n\n# Language input (one-hot encoded)\nlang_input = tf.keras.layers.Input(shape=(num_languages,), name='language')\n\n# Concatenate text embeddings with language input\nconcat = tf.keras.layers.Concatenate()([text_embeddings, lang_input])\n\n# Classification head\ndense = tf.keras.layers.Dense(128, activation='relu')(concat)\noutput = tf.keras.layers.Dense(1, activation='sigmoid')(dense)  # Binary classification\n\n# Define the model\nmodel = tf.keras.Model(inputs=[input_ids, attention_mask, lang_input], outputs=output)\n\n# Compile the model\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:29:36.572201Z","iopub.execute_input":"2024-09-23T21:29:36.572874Z","iopub.status.idle":"2024-09-23T21:30:01.066229Z","shell.execute_reply.started":"2024-09-23T21:29:36.572835Z","shell.execute_reply":"2024-09-23T21:30:01.065367Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nAll the weights of TFBertModel were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"history = model.fit(\n    [train_input_ids,train_encodings_attention, train_langs_encoded],  # Inputs: text + language\n    train_labels,  # Labels\n    validation_data=([dev_input_ids, dev_encodings_attention, dev_langs_encoded], dev_labels),  # Validation\n    epochs=1,\n    batch_size=32\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-23T21:30:15.834980Z","iopub.execute_input":"2024-09-23T21:30:15.835507Z","iopub.status.idle":"2024-09-24T00:59:47.842630Z","shell.execute_reply.started":"2024-09-23T21:30:15.835456Z","shell.execute_reply":"2024-09-24T00:59:47.841602Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1727127081.878221     127 service.cc:145] XLA service 0x7cf251aa6340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1727127081.878270     127 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1727127082.046133     127 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"21066/21066 [==============================] - 12572s 593ms/step - loss: 0.1937 - accuracy: 0.9175 - val_loss: 0.1912 - val_accuracy: 0.9228\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Load the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n\n# External test data: manually written sentences and their corresponding language codes\ntest_texts = [\"by the way i feel i am not well\", \"OpenCellular (OC) ist eine Open-Source-Zugangsplattform mit Schwerpunkt auf ländlicher Konnektivität\"]  # Example test sentences (English and German)\ntest_langs = [\"en\", \"de\"]  # Corresponding language codes\n\n# Step 1: Tokenize the text input (manually written sentences) for BERT\nmax_length = 128  # Ensure this matches the max_length used during training\ntest_encodings = tokenizer(\n    test_texts,\n    max_length=max_length,\n    padding='max_length',  # This ensures all sequences are padded to max_length\n    truncation=True,  # Truncates sentences longer than max_length\n    return_tensors=\"tf\"  # Returns the tensors needed for the model\n)\n\n# Extract tokenized input IDs and attention masks\ntest_input_ids = test_encodings['input_ids']  # Shape: (batch_size, max_length)\ntest_attention_mask = test_encodings['attention_mask']  # Shape: (batch_size, max_length)\n\n# Step 2: One-hot encode the language input\nencoder = OneHotEncoder(sparse=False)  # If not already fitted, load from training step\n# Fit encoder if it wasn't fitted earlier\nencoder.fit(np.array(train_langs).reshape(-1, 1))  # Fitting it on the training languages, if needed\n\n# One-hot encode the manually written language inputs\ntest_langs_encoded = encoder.transform(np.array(test_langs).reshape(-1, 1))  # Shape: (batch_size, num_languages)\n\n# Print shapes for debugging\nprint(f\"test_input_ids shape: {test_input_ids.shape}\")  # Should be (batch_size, max_length)\nprint(f\"test_attention_mask shape: {test_attention_mask.shape}\")  # Should be (batch_size, max_length)\nprint(f\"test_langs_encoded shape: {test_langs_encoded.shape}\")  # Should be (batch_size, num_languages)\n\n# Step 3: Load the trained model\n\n# Step 4: Ensure all inputs are compatible for prediction\n# The model expects [input_ids, attention_mask, one-hot encoded language input]\n\n# Check if the batch size is correct\nif test_input_ids.shape[0] == test_attention_mask.shape[0] == test_langs_encoded.shape[0]:\n    # Step 5: Make predictions\n    test_predictions = model.predict([test_input_ids, test_attention_mask, test_langs_encoded])\n    \n    # Step 6: Interpret predictions\n    # Assuming binary classification (0 or 1)\n    predicted_labels = (test_predictions > 0.5).astype(int)\n\n    # Print predictions for each test sentence\n    for i, (text, lang, prediction) in enumerate(zip(test_texts, test_langs, predicted_labels)):\n        label = 'Machine-Generated' if prediction == 1 else 'Human-Generated'\n        print(f\"Text: {text}\")\n        print(f\"Language: {lang}\")\n        print(f\"Predicted Label: {label}\\n\")\nelse:\n    print(\"Input shapes are incompatible for prediction. Please check the input data.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-24T01:13:25.569266Z","iopub.execute_input":"2024-09-24T01:13:25.569938Z","iopub.status.idle":"2024-09-24T01:13:26.453144Z","shell.execute_reply.started":"2024-09-24T01:13:25.569894Z","shell.execute_reply":"2024-09-24T01:13:26.452275Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"test_input_ids shape: (2, 128)\ntest_attention_mask shape: (2, 128)\ntest_langs_encoded shape: (2, 9)\n1/1 [==============================] - 0s 86ms/step\nText: by the way i feel i am not well\nLanguage: en\nPredicted Label: Human-Generated\n\nText: OpenCellular (OC) ist eine Open-Source-Zugangsplattform mit Schwerpunkt auf ländlicher Konnektivität\nLanguage: de\nPredicted Label: Machine-Generated\n\n","output_type":"stream"}]}]}