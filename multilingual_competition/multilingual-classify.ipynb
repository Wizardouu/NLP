{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:28:25.576407Z",
     "iopub.status.busy": "2024-09-23T21:28:25.575736Z",
     "iopub.status.idle": "2024-09-23T21:28:45.418546Z",
     "shell.execute_reply": "2024-09-23T21:28:45.417742Z",
     "shell.execute_reply.started": "2024-09-23T21:28:25.576367Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e7e446aa3f461b9ef45a8f59631b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/589 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda24c36390e4fca8eeb89a029938969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cae3ba1454b49d9bd8d3bd18dc75494",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/213M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef334ea20e441198c688e4702d18fc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/212M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d12def2fb24042f7a606c0cb5dbe456f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/275M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426d96ebd2234389a9e82aad459b33be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/674083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b05138f23bd4b92812eb062873cdd52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating dev split:   0%|          | 0/288894 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Jinyan1/COLING_2025_MGT_multingual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:28:45.420601Z",
     "iopub.status.busy": "2024-09-23T21:28:45.420119Z",
     "iopub.status.idle": "2024-09-23T21:28:45.425801Z",
     "shell.execute_reply": "2024-09-23T21:28:45.424683Z",
     "shell.execute_reply.started": "2024-09-23T21:28:45.420567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
      "        num_rows: 674083\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['id', 'source', 'sub_source', 'lang', 'model', 'label', 'text'],\n",
      "        num_rows: 288894\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:28:45.427710Z",
     "iopub.status.busy": "2024-09-23T21:28:45.426953Z",
     "iopub.status.idle": "2024-09-23T21:28:49.172534Z",
     "shell.execute_reply": "2024-09-23T21:28:49.171521Z",
     "shell.execute_reply.started": "2024-09-23T21:28:45.427669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id source   sub_source lang        model  \\\n",
      "0  808a846d-41c1-46ff-80ab-892b01bec5f3   m4gt        arxiv   en  gemma-7b-it   \n",
      "1  673ec6fc-624a-4ad5-8d5f-a5950b0e5432   mage           wp   en        human   \n",
      "2  3059e6e4-1724-4745-a833-cbb6bfb4acd8   mage         xsum   en   flan_t5_xl   \n",
      "3  4ca1cde7-6dbd-4519-b60c-7f8c8fcd0d36    hc3      open_qa   zh       gpt-35   \n",
      "4  f9cca231-ed31-45a9-a448-99245f72e32c    hc3  reddit_eli5   en       gpt-35   \n",
      "\n",
      "   label                                               text  \n",
      "0      1  This report summarizes the findings of the US ...  \n",
      "1      0  I've been standing here for days now. Watching...  \n",
      "2      1  Towell, 25, was knocked down twice during the ...  \n",
      "3      1  我不确定你想要问的是什么，但是阿玛尼是一个奢侈品牌，它的产品包括化妆品、香水和时装。黑手党是...  \n",
      "4      1  Sometimes when we eat certain types of food, o...  \n",
      "                                     id source        sub_source lang   model  \\\n",
      "0  8a6ce8d6-a07c-4a2b-831e-db606a7d46ad   mage                wp   en      7B   \n",
      "1  e811404e-fd66-4e58-a2d4-e0e3d5a4b38b   mage              eli5   en   human   \n",
      "2  963c8b72-a7d1-4af8-babd-20dc67248e6d    hc3       reddit_eli5   en   human   \n",
      "3  c9bf362f-6329-405d-9b86-3a6312179eb3   m4gt  True & Fake News   bg   human   \n",
      "4  a61f9bd7-58ec-4339-ac15-c107d2a0f9e8   mage             squad   en  t0_11b   \n",
      "\n",
      "   label                                               text  \n",
      "0      1  The United Nations declared war has been illeg...  \n",
      "1      0  Horse bones are incredibly dense and under a m...  \n",
      "2      0  Balls originally form in the abdomen . And eve...  \n",
      "3      0  Несъгласие с предложенията на служебния минист...  \n",
      "4      1  Temujin began his ascent to power by offering ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "train_df = ds['train'].to_pandas()\n",
    "dev_df = ds['dev'].to_pandas()\n",
    "\n",
    "# Verify the conversion\n",
    "print(train_df.head())\n",
    "print(dev_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:28:49.176759Z",
     "iopub.status.busy": "2024-09-23T21:28:49.176457Z",
     "iopub.status.idle": "2024-09-23T21:29:17.719130Z",
     "shell.execute_reply": "2024-09-23T21:29:17.718268Z",
     "shell.execute_reply.started": "2024-09-23T21:28:49.176726Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7b366a072d4b80844019c2187b9eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b35892425d4e24b3f398f58f20ca61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c01b4325c864f1180f2cb9438bccc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6906c4670d64df6b230ad29c32b66fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5615dda9a70e474ba90c59b4d99dc1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "\n",
    "train_texts = train_df['text'].tolist()\n",
    "train_langs = train_df['lang'].tolist()\n",
    "train_labels = train_df['label'].values\n",
    "\n",
    "dev_texts = dev_df['text'].tolist()\n",
    "dev_langs = dev_df['lang'].tolist()\n",
    "dev_labels = dev_df['label'].values\n",
    "\n",
    "max_length = 128  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': []\n",
    "}\n",
    "\n",
    "for text in tqdm(train_texts, desc=\"Tokenizing train data\"):\n",
    "    enc = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
    "    train_encodings['input_ids'].append(enc['input_ids'])\n",
    "    train_encodings['attention_mask'].append(enc['attention_mask'])\n",
    "\n",
    "train_encodings['input_ids'] = tf.concat(train_encodings['input_ids'], axis=0)\n",
    "train_encodings['attention_mask'] = tf.concat(train_encodings['attention_mask'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T12:48:03.685453Z",
     "iopub.status.busy": "2024-09-23T12:48:03.684989Z",
     "iopub.status.idle": "2024-09-23T12:48:03.692932Z",
     "shell.execute_reply": "2024-09-23T12:48:03.691438Z",
     "shell.execute_reply.started": "2024-09-23T12:48:03.685413Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674083\n"
     ]
    }
   ],
   "source": [
    "print(len(train_encodings['attention_mask']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T15:05:28.508192Z",
     "iopub.status.busy": "2024-09-23T15:05:28.505047Z",
     "iopub.status.idle": "2024-09-23T15:52:47.449615Z",
     "shell.execute_reply": "2024-09-23T15:52:47.448024Z",
     "shell.execute_reply.started": "2024-09-23T15:05:28.508080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing dev data: 100%|██████████| 288894/288894 [46:33<00:00, 103.43it/s]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "\n",
    "dev_encodings = {\n",
    "    'input_ids': [],\n",
    "    'attention_mask': []\n",
    "}\n",
    "\n",
    "for text in tqdm(dev_texts, desc=\"Tokenizing dev data\"):\n",
    "    enc = tokenizer(text, truncation=True, padding='max_length', max_length=max_length, return_tensors='tf')\n",
    "    dev_encodings['input_ids'].append(enc['input_ids'])\n",
    "    dev_encodings['attention_mask'].append(enc['attention_mask'])\n",
    "\n",
    "dev_encodings['input_ids'] = tf.concat(dev_encodings['input_ids'], axis=0)\n",
    "dev_encodings['attention_mask'] = tf.concat(dev_encodings['attention_mask'], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:29:17.721580Z",
     "iopub.status.busy": "2024-09-23T21:29:17.720881Z",
     "iopub.status.idle": "2024-09-23T21:29:25.691329Z",
     "shell.execute_reply": "2024-09-23T21:29:25.689905Z",
     "shell.execute_reply.started": "2024-09-23T21:29:17.721543Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/kaggle/input/train-ids/input_ids.pkl', 'rb') as file:\n",
    "    train_input_ids = pickle.load(file)\n",
    "with open('/kaggle/input/train-attention/attention_masks.pkl', 'rb') as file:\n",
    "    train_encodings_attention = pickle.load(file)\n",
    "with open('/kaggle/input/dev-attention/dev_attention.pkl', 'rb') as file:\n",
    "    dev_encodings_attention = pickle.load(file)\n",
    "with open('/kaggle/input/dev-input-ids/dev_input_ids.pkl', 'rb') as file:\n",
    "    dev_input_ids = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:29:25.693665Z",
     "iopub.status.busy": "2024-09-23T21:29:25.693272Z",
     "iopub.status.idle": "2024-09-23T21:29:27.227324Z",
     "shell.execute_reply": "2024-09-23T21:29:27.226482Z",
     "shell.execute_reply.started": "2024-09-23T21:29:25.693619Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(sparse = False)\n",
    "train_langs_encoded = encoder.fit_transform(np.array(train_langs).reshape(-1, 1))\n",
    "dev_langs_encoded = encoder.transform(np.array(dev_langs).reshape(-1, 1))\n",
    "\n",
    "num_languages = train_langs_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:29:36.572874Z",
     "iopub.status.busy": "2024-09-23T21:29:36.572201Z",
     "iopub.status.idle": "2024-09-23T21:30:01.066229Z",
     "shell.execute_reply": "2024-09-23T21:30:01.065367Z",
     "shell.execute_reply.started": "2024-09-23T21:29:36.572835Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_mask = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "bert_outputs = bert_model(input_ids, attention_mask=attention_mask)\n",
    "text_embeddings = tf.keras.layers.GlobalAveragePooling1D()(bert_outputs.last_hidden_state)\n",
    "\n",
    "lang_input = tf.keras.layers.Input(shape=(num_languages,), name='language')\n",
    "\n",
    "concat = tf.keras.layers.Concatenate()([text_embeddings, lang_input])\n",
    "\n",
    "dense = tf.keras.layers.Dense(128, activation='relu')(concat)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask, lang_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T21:30:15.835507Z",
     "iopub.status.busy": "2024-09-23T21:30:15.834980Z",
     "iopub.status.idle": "2024-09-24T00:59:47.842630Z",
     "shell.execute_reply": "2024-09-24T00:59:47.841602Z",
     "shell.execute_reply.started": "2024-09-23T21:30:15.835456Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727127081.878221     127 service.cc:145] XLA service 0x7cf251aa6340 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1727127081.878270     127 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1727127082.046133     127 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21066/21066 [==============================] - 12572s 593ms/step - loss: 0.1937 - accuracy: 0.9175 - val_loss: 0.1912 - val_accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [train_input_ids,train_encodings_attention, train_langs_encoded],  \n",
    "    train_labels, \n",
    "    validation_data=([dev_input_ids, dev_encodings_attention, dev_langs_encoded], dev_labels), \n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-24T01:13:25.569938Z",
     "iopub.status.busy": "2024-09-24T01:13:25.569266Z",
     "iopub.status.idle": "2024-09-24T01:13:26.453144Z",
     "shell.execute_reply": "2024-09-24T01:13:26.452275Z",
     "shell.execute_reply.started": "2024-09-24T01:13:25.569894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_input_ids shape: (2, 128)\n",
      "test_attention_mask shape: (2, 128)\n",
      "test_langs_encoded shape: (2, 9)\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "Text: by the way i feel i am not well\n",
      "Language: en\n",
      "Predicted Label: Human-Generated\n",
      "\n",
      "Text: OpenCellular (OC) ist eine Open-Source-Zugangsplattform mit Schwerpunkt auf ländlicher Konnektivität\n",
      "Language: de\n",
      "Predicted Label: Machine-Generated\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "test_texts = [\"by the way i feel i am not well\", \"OpenCellular (OC) ist eine Open-Source-Zugangsplattform mit Schwerpunkt auf ländlicher Konnektivität\"]  # Example test sentences (English and German)\n",
    "test_langs = [\"en\", \"de\"]  \n",
    "\n",
    "max_length = 128 \n",
    "test_encodings = tokenizer(\n",
    "    test_texts,\n",
    "    max_length=max_length,\n",
    "    padding='max_length',  \n",
    "    truncation=True, \n",
    "    return_tensors=\"tf\"  \n",
    ")\n",
    "\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_attention_mask = test_encodings['attention_mask']  \n",
    "\n",
    "encoder = OneHotEncoder(sparse=False) \n",
    "encoder.fit(np.array(train_langs).reshape(-1, 1))\n",
    "\n",
    "test_langs_encoded = encoder.transform(np.array(test_langs).reshape(-1, 1))\n",
    "\n",
    "print(f\"test_input_ids shape: {test_input_ids.shape}\")\n",
    "print(f\"test_attention_mask shape: {test_attention_mask.shape}\")  \n",
    "print(f\"test_langs_encoded shape: {test_langs_encoded.shape}\")  \n",
    "\n",
    "if test_input_ids.shape[0] == test_attention_mask.shape[0] == test_langs_encoded.shape[0]:\n",
    "    test_predictions = model.predict([test_input_ids, test_attention_mask, test_langs_encoded])\n",
    "    \n",
    "    predicted_labels = (test_predictions > 0.5).astype(int)\n",
    "\n",
    "    for i, (text, lang, prediction) in enumerate(zip(test_texts, test_langs, predicted_labels)):\n",
    "        label = 'Machine-Generated' if prediction == 1 else 'Human-Generated'\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Language: {lang}\")\n",
    "        print(f\"Predicted Label: {label}\\n\")\n",
    "else:\n",
    "    print(\"Input shapes are incompatible for prediction. Please check the input data.\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5754014,
     "sourceId": 9463639,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5754115,
     "sourceId": 9463778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5754691,
     "sourceId": 9464571,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5754745,
     "sourceId": 9464638,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 197956292,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
